{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base functions (make fig & prep data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "make fig\n",
    "'''\n",
    "def plot_sol1(X_star, phi1,title):\n",
    "    lb = X_star.min(0); ub = X_star.max(0)\n",
    "    x, y = np.linspace(lb[0], ub[0], 200), np.linspace(lb[1], ub[1], 150); x, y = np.meshgrid(x, y)\n",
    "    PHI_I = griddata(X_star, phi1.flatten(), (x, y), method = \"linear\")\n",
    "    plt.figure(figsize = (12, 4))\n",
    "    plt.imshow(PHI_I, interpolation='nearest',cmap='rainbow', extent=[0,1,-1,1], origin='lower', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('x')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    plt.savefig(f'./pics/{title}')\n",
    "\n",
    "def plot_loss_log(ep_log, loss_log,save_name):\n",
    "    plt.figure(figsize = (8, 4))\n",
    "    plt.plot(ep_log, loss_log, alpha = .7, label = \"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(alpha = .5)\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.title('train loss(log)')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    plt.savefig(f'./pics/{save_name}')\n",
    "\n",
    "def plot_loss(ep_log, loss_log,save_name):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    fig,ax = plt.subplots(2,1)\n",
    "    strt = int(len(ep_log)*0.6)\n",
    "    ax[0].plot(ep_log, loss_log)\n",
    "    ax[0].grid(alpha=.5)\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_title('train loss')\n",
    "    ax[1].plot(ep_log[strt:], loss_log[strt:])\n",
    "    ax[1].grid(alpha=.5)\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    fig.savefig(f\"./pics/{save_name}\")\n",
    "\n",
    "\"\"\"\n",
    "prep data\n",
    "\"\"\"\n",
    "def func_u0(x):\n",
    "    return - tf.sin(np.pi * x)\n",
    "\n",
    "def func_ub(x):\n",
    "    n = x.shape[0]\n",
    "    return tf.zeros((n, 1), dtype = tf.float32)\n",
    "\n",
    "def prp_grd(tmin, tmax, nt,\n",
    "            xmin, xmax, nx):\n",
    "    t = np.linspace(tmin, tmax, nt)\n",
    "    x = np.linspace(xmin, xmax, nx)\n",
    "    t, x = np.meshgrid(t, x)\n",
    "    t, x = t.reshape(-1, 1), x.reshape(-1, 1)\n",
    "    TX = np.c_[t, x]\n",
    "    return t, x, TX\n",
    "\n",
    "def prp_dataset(tmin, tmax, xmin, xmax, N_0, N_b, N_r):\n",
    "    lb = tf.constant([tmin, xmin], dtype = tf.float32)\n",
    "    ub = tf.constant([tmax, xmax], dtype = tf.float32)\n",
    "    print(\"lower bound\", lb)\n",
    "    print(\"upper bound\", ub)\n",
    "\n",
    "    t_0 = tf.ones((N_0, 1), dtype = tf.float32) * lb[0]\n",
    "    x_0 = tf.random.uniform((N_0, 1), lb[1], ub[1], dtype = tf.float32)\n",
    "    t_b = tf.random.uniform((N_b, 1), lb[0], ub[0], dtype = tf.float32)\n",
    "    x_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b, 1), .5, dtype = tf.float32)\n",
    "    t_r = tf.random.uniform((N_r, 1), lb[0], ub[0], dtype = tf.float32)\n",
    "    x_r = tf.random.uniform((N_r, 1), lb[1], ub[1], dtype = tf.float32)\n",
    "\n",
    "    return t_0, x_0, t_b, x_b, t_r, x_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PINNs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 t_0, x_0, u_0, \n",
    "                 t_b, x_b, u_b, \n",
    "                 t_f, x_f, \n",
    "                 in_dim, out_dim, width, depth, activ = \"tanh\", \n",
    "                 w_init = \"glorot_normal\", b_init = \"zeros\", \n",
    "                 lr = 1e-3, opt = \"Adam\",\n",
    "                 freq_info = 10, r_seed = 1234):\n",
    "        # initialize the configuration\n",
    "        super().__init__()\n",
    "        self.r_seed = r_seed\n",
    "        self.random_seed(seed = r_seed)\n",
    "        self.data_type  = tf.float32\n",
    "        self.in_dim     = in_dim       # input dimension\n",
    "        self.out_dim     = out_dim       # output dimension\n",
    "        self.width     = width       # internal dimension\n",
    "        self.depth  = depth    # (# of hidden layers) + output layer\n",
    "        self.activ  = activ    # activation function\n",
    "        self.w_init = w_init   # initial weight\n",
    "        self.b_init = b_init   # initial bias\n",
    "        self.lr     = lr       # learning rate\n",
    "        self.opt    = opt      # name of your optimizer\n",
    "        self.freq_info = freq_info    # monitoring frequency\n",
    "\n",
    "        # input-output pair\n",
    "        self.t_0 = t_0; self.x_0 = x_0; self.u_0 = u_0   # evaluates initial condition\n",
    "        self.t_b = t_b; self.x_b = x_b; self.u_b = u_b   # evaluates boundary condition\n",
    "        self.t_f = t_f; self.x_f = x_f                   # evaluates domain residual\n",
    "        \n",
    "        # bounds\n",
    "        X_r     = tf.concat([t_f, x_f], 1)\n",
    "        self.lb = tf.cast(tf.reduce_min(X_r, axis = 0), self.data_type)\n",
    "        self.ub = tf.cast(tf.reduce_max(X_r, axis = 0), self.data_type)\n",
    "        \n",
    "        # call\n",
    "        self.dnn = self.dnn_init(in_dim, out_dim, width, depth)\n",
    "        self.params = self.dnn.trainable_variables\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = self.lr, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "        \n",
    "        # parameter setting\n",
    "        self.nu = tf.constant(.01 / np.pi, dtype = self.data_type)\n",
    "\n",
    "        # track loss\n",
    "        self.ep_log = []\n",
    "        self.loss_log = []\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"****************     MAIN PROGRAM START     ****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> start time:\", datetime.datetime.now())\n",
    "        print(\">>>>> configuration;\")\n",
    "        print(\"         dtype        :\", self.data_type)\n",
    "        print(\"         activ func   :\", self.activ)\n",
    "        print(\"         weight init  :\", self.w_init)\n",
    "        print(\"         learning rate:\", self.lr)\n",
    "        print(\"         optimizer    :\", self.opt)\n",
    "        print(\"         summary      :\", self.dnn.summary())\n",
    "        \n",
    "    def random_seed(self, seed = 1234):\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        \n",
    "    def dnn_init(self, in_dim, out_dim, width, depth):\n",
    "        # network configuration (N: in_dim -> out_dim (in_dim -> width -> ... -> width -> out_dim))\n",
    "        network = tf.keras.Sequential()\n",
    "        network.add(tf.keras.layers.InputLayer(in_dim))\n",
    "        network.add(tf.keras.layers.Lambda(lambda x: 2. * (x - self.lb) / (self.ub - self.lb) - 1.))\n",
    "        # construct the network\n",
    "        for l in range(depth - 1):\n",
    "            network.add(tf.keras.layers.Dense(width, activation = self.activ, use_bias = True,\n",
    "                                                kernel_initializer = self.w_init, bias_initializer = self.b_init, \n",
    "                                                kernel_regularizer = None, bias_regularizer = None, \n",
    "                                                activity_regularizer = None, kernel_constraint = None, bias_constraint = None))\n",
    "        network.add(tf.keras.layers.Dense(out_dim))\n",
    "        return network\n",
    "    \n",
    "    def PDE(self, t, x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            tp.watch(t)\n",
    "            tp.watch(x)\n",
    "            u = self.dnn(tf.concat([t, x], 1))\n",
    "            u_x = tp.gradient(u, x)\n",
    "        u_t  = tp.gradient(u, t)\n",
    "        u_xx = tp.gradient(u_x, x)\n",
    "        del tp\n",
    "        f = u_t + u * u_x - self.nu * u_xx\n",
    "        return u, f\n",
    "\n",
    "    @tf.function\n",
    "    def loss_glb(self, \n",
    "                 t_0, x_0, u_0, \n",
    "                 t_b, x_b, u_b, \n",
    "                 t_f, x_f):\n",
    "        loss_0_u_hat,loss_0_f = self.PDE(t_0, x_0)\n",
    "        loss_b_u_hat,loss_b_f = self.PDE(t_b, x_b)\n",
    "        f = self.PDE(t_f,x_f)\n",
    "        loss_0 = tf.reduce_mean(tf.square(u_0 - loss_0_u_hat))\n",
    "        loss_b = tf.reduce_mean(tf.square(u_b - loss_b_u_hat))\n",
    "        loss_f = tf.reduce_mean(tf.square(f))\n",
    "        loss_glb = loss_0 + loss_b + loss_f\n",
    "        return loss_glb\n",
    "\n",
    "    def loss_grad(self, \n",
    "                  t_0, x_0, u_0, \n",
    "                  t_b, x_b, u_b, \n",
    "                  t_f, x_f): \n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            loss = self.loss_glb(t_0, x_0, u_0, \n",
    "                                 t_b, x_b, u_b, \n",
    "                                 t_f, x_f)\n",
    "        grad = tp.gradient(loss, self.params)\n",
    "        del tp\n",
    "        return loss, grad\n",
    "    \n",
    "    @tf.function\n",
    "    def grad_desc(self, \n",
    "                  t_0, x_0, u_0, \n",
    "                  t_b, x_b, u_b, \n",
    "                  t_f, x_f):\n",
    "        loss, grad = self.loss_grad(t_0, x_0, u_0, \n",
    "                                    t_b, x_b, u_b, \n",
    "                                    t_f, x_f)\n",
    "        self.optimizer.apply_gradients(zip(grad, self.params))\n",
    "        return loss\n",
    "        \n",
    "    def train(self, epoch = 10 ** 5, batch = 2 ** 6, tol = 1e-5): \n",
    "        print(\">>>>> training setting;\")\n",
    "        print(\"         # of epoch     :\", epoch)\n",
    "        print(\"         batch size     :\", batch)\n",
    "        print(\"         convergence tol:\", tol)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # I had to convert input data (tf.tensor) into numpy style in order for mini-batch training (slicing)\n",
    "        # and this works well for both full-batch and mini-batch training\n",
    "        t_0 = self.t_0.numpy(); x_0 = self.x_0.numpy(); u_0 = self.u_0.numpy()\n",
    "        t_b = self.t_b.numpy(); x_b = self.x_b.numpy(); u_b = self.u_b.numpy()\n",
    "        t_f = self.t_f.numpy(); x_f = self.x_f.numpy()\n",
    "        \n",
    "        for ep in range(epoch):\n",
    "            ep_loss = 0\n",
    "            n_r = self.x_f.shape[0]\n",
    "            idx_f = np.random.permutation(n_r)\n",
    "            for idx in range(0, n_r, batch):\n",
    "                # batch for domain residual\n",
    "                t_f_btch = tf.convert_to_tensor(t_f[idx_f[idx: idx + batch if idx + batch < n_r else n_r]], dtype = self.data_type)\n",
    "                x_f_btch = tf.convert_to_tensor(x_f[idx_f[idx: idx + batch if idx + batch < n_r else n_r]], dtype = self.data_type)\n",
    "                # compute loss and perform gradient descent\n",
    "                loss_btch = self.grad_desc(t_0, x_0, u_0, \n",
    "                                            t_b, x_b, u_b, \n",
    "                                            t_f_btch, x_f_btch)\n",
    "                ep_loss += loss_btch / int(n_r / batch)\n",
    "                \n",
    "            if ep % self.freq_info == 0:\n",
    "                elps = time.time() - t0\n",
    "                self.ep_log.append(ep)\n",
    "                self.loss_log.append(ep_loss)\n",
    "                print(\"ep: %d, loss: %.3e, elps: %.3f\" % (ep, ep_loss, elps))\n",
    "                t0 = time.time()\n",
    "            \n",
    "            if ep_loss < tol:\n",
    "                print(\">>>>> program terminating with the loss converging to its tolerance.\")\n",
    "                print(\"\\n************************************************************\")\n",
    "                print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "                print(\"************************************************************\")\n",
    "                print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                break\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                \n",
    "    def predict(self, t, x):\n",
    "        u_hat, f = self.PDE(t, x)\n",
    "        return u_hat, f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### burgers equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "    tmin, tmax =  0., 1.\n",
    "    xmin, xmax = -1., 1.\n",
    "    N_0 = 60\n",
    "    N_b = 60\n",
    "    N_r = 1500\n",
    "\n",
    "    epoch = 30000\n",
    "    batch = 2**12\n",
    "    tol = 1e-8\n",
    "\n",
    "    lr0 = 5e-3\n",
    "    gam = 1e-2\n",
    "    lrd_cos = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate = lr0, \n",
    "        decay_steps = epoch, \n",
    "        alpha = gam\n",
    "        )\n",
    "    lr = lrd_cos\n",
    "\n",
    "    t_0, x_0, t_b, x_b, t_r, x_r = prp_dataset(tmin, tmax, xmin, xmax, N_0, N_b, N_r)\n",
    "    u_0 = func_u0(x_0)\n",
    "    u_b = func_ub(x_b)\n",
    "\n",
    "    pinn = PINN(t_0, x_0, u_0, \n",
    "                t_b, x_b, u_b, \n",
    "                t_r, x_r, \n",
    "                in_dim = 2, out_dim=1, width=20, depth=8, activ = \"tanh\",\n",
    "                w_init = \"glorot_normal\", b_init = \"zeros\", \n",
    "                lr = lr, opt = \"Adam\")\n",
    "\n",
    "    # if os.path.exists('./burgers_saved_model'):\n",
    "    #     # 只保留模型dnn，pinn中的其他都未保存\n",
    "    #     print('load the previous model')\n",
    "    #     pinn.dnn = tf.saved_model.load('./burgers_saved_model')\n",
    "    # else:\n",
    "    pinn.train(epoch, batch, tol)\n",
    "    plot_loss_log(pinn.ep_log,pinn.loss_log,'train_loss_log')\n",
    "    plot_loss(pinn.ep_log,pinn.loss_log,'train_loss')\n",
    "        \n",
    "    # PINN inference\n",
    "    nt = int(1e3) + 1\n",
    "    nx = int(1e2) + 1\n",
    "    t, x, TX = prp_grd(\n",
    "        tmin, tmax, nt, \n",
    "        xmin, xmax, nx\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    u_hat, f = pinn.predict(t, x)\n",
    "    t1 = time.time()\n",
    "    elps = t1 - t0\n",
    "    print(\"elapsed time for PINN inference (sec):\", elps)\n",
    "    print(\"elapsed time for PINN inference (min):\", elps / 60.)\n",
    "    plot_sol1(TX, u_hat.numpy(),title='prediction')\n",
    "    plot_sol1(TX, f.numpy(),title='Euler equation loss')\n",
    "\n",
    "    # pinn.dnn.save(\"burgers_saved_model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
