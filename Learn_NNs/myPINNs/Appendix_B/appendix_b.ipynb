{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "'''\n",
    "make figure\n",
    "'''\n",
    "def plot_sol1(X_star, phi1,title):\n",
    "    lb = X_star.min(0); ub = X_star.max(0)\n",
    "    x, y = np.linspace(lb[0], ub[0], 200), np.linspace(lb[1], ub[1], 150); x, y = np.meshgrid(x, y)\n",
    "    PHI_I = griddata(X_star, phi1.flatten(), (x, y), method = \"linear\")\n",
    "    plt.figure(figsize = (6, 8))\n",
    "    plt.imshow(PHI_I, interpolation='nearest',cmap='rainbow', extent=[0,1,-1,1], origin='lower', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('x')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    plt.savefig(f'./pics/{title}')\n",
    "\n",
    "def plot_loss_log(ep_log, loss_log,save_name):\n",
    "    plt.figure(figsize = (8, 4))\n",
    "    plt.plot(ep_log, loss_log, alpha = .7, label = \"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(alpha = .5)\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.title('train loss(log)')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    plt.savefig(f'./pics/{save_name}')\n",
    "\n",
    "def plot_loss(ep_log, loss_log,save_name):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    fig,ax = plt.subplots(2,1)\n",
    "    strt = int(len(ep_log)*0.6)\n",
    "    ax[0].plot(ep_log, loss_log)\n",
    "    ax[0].grid(alpha=.5)\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_title('train loss')\n",
    "    ax[1].plot(ep_log[strt:], loss_log[strt:])\n",
    "    ax[1].grid(alpha=.5)\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    fig.savefig(f\"./pics/{save_name}\")\n",
    "'''\n",
    "prep data\n",
    "'''\n",
    "def func_up(t,x):\n",
    "    return tf.ones((len(t.numpy()),1), dtype = tf.float32)\n",
    "\n",
    "def func_ub(x):\n",
    "    n = x.shape[0]\n",
    "    return tf.zeros((n, 1), dtype = tf.float32)\n",
    "\n",
    "def prp_grd(tmin, tmax, nt,\n",
    "            xmin, xmax, nx):\n",
    "    t = np.linspace(tmin, tmax, nt)\n",
    "    x = np.linspace(xmin, xmax, nx)\n",
    "    t, x = np.meshgrid(t, x)\n",
    "    t, x = t.reshape(-1, 1), x.reshape(-1, 1)\n",
    "    TX = np.c_[t, x]\n",
    "    return t, x, TX\n",
    "\n",
    "def prp_dataset(tmin, tmax, xmin, xmax, N_nabla_rho,N_p,x_star,N_f):\n",
    "    t_nabla_rho = tf.random.uniform((N_nabla_rho, 1), tmin, tmax, dtype = tf.float32)\n",
    "    x_nabla_rho = tf.random.uniform((N_nabla_rho,1), xmin, xmax, dtype = tf.float32)\n",
    "    t_p = tf.random.uniform((N_p, 1), tmin, tmax, dtype = tf.float32)\n",
    "    x_p = tf.ones((N_p,1),dtype=tf.float32) * x_star\n",
    "    t_f = tf.random.uniform((N_f, 1), tmin, tmax, dtype = tf.float32)\n",
    "    x_f = tf.random.uniform((N_f,1), xmin, xmax, dtype = tf.float32)\n",
    "\n",
    "    return t_nabla_rho, x_nabla_rho, t_p, x_p, t_f, x_f\n",
    "\n",
    "\"\"\"\n",
    "PINNs model\n",
    "\"\"\"\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 t_nabla_rho, x_nabla_rho,\n",
    "                 t_p, x_p, u_p,\n",
    "                 t_f, x_f,\n",
    "                 in_dim, out_dim, width, depth, activ = \"tanh\", \n",
    "                 w_init = \"glorot_normal\", b_init = \"zeros\", \n",
    "                 lr = 1e-3, opt = \"Adam\",\n",
    "                 freq_info = 100, r_seed = 1234):\n",
    "        # initialize the configuration\n",
    "        super().__init__()\n",
    "        self.r_seed = r_seed\n",
    "        self.random_seed(seed = r_seed)\n",
    "        self.data_type  = tf.float32\n",
    "        self.in_dim     = in_dim       # input dimension\n",
    "        self.out_dim     = out_dim       # output dimension\n",
    "        self.width     = width       # internal dimension\n",
    "        self.depth  = depth    # (# of hidden layers) + output layer\n",
    "        self.activ  = activ    # activation function\n",
    "        self.w_init = w_init   # initial weight\n",
    "        self.b_init = b_init   # initial bias\n",
    "        self.lr     = lr       # learning rate\n",
    "        self.opt    = opt      # name of your optimizer\n",
    "        self.freq_info = freq_info    # monitoring frequency\n",
    "\n",
    "        # input-output pair\n",
    "        self.t_nabla_rho = t_nabla_rho; self.x_nabla_rho = x_nabla_rho   # evaluates initial condition\n",
    "        self.t_p = t_p; self.x_p = x_p; self.u_p = u_p# evaluates boundary condition\n",
    "        self.t_f = t_f; self.x_f = x_f                   # evaluates domain residual\n",
    "        \n",
    "        # bounds\n",
    "        X_f     = tf.concat([t_f, x_f], 1)\n",
    "        self.lb = tf.cast(tf.reduce_min(X_f, axis = 0), self.data_type)\n",
    "        self.ub = tf.cast(tf.reduce_max(X_f, axis = 0), self.data_type)\n",
    "        \n",
    "        # call\n",
    "        self.dnn = self.dnn_init(in_dim, out_dim, width, depth)\n",
    "        self.params = self.dnn.trainable_variables\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = self.lr, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "        \n",
    "        # parameter setting\n",
    "        self.nu = tf.constant(.01 / np.pi, dtype = self.data_type)\n",
    "\n",
    "        # track loss\n",
    "        self.ep_log = []\n",
    "        self.loss_log = []\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"****************     MAIN PROGRAM START     ****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> start time:\", datetime.datetime.now())\n",
    "        print(\">>>>> configuration;\")\n",
    "        print(\"         dtype        :\", self.data_type)\n",
    "        print(\"         activ func   :\", self.activ)\n",
    "        print(\"         weight init  :\", self.w_init)\n",
    "        print(\"         learning rate:\", self.lr)\n",
    "        print(\"         optimizer    :\", self.opt)\n",
    "        print(\"         summary      :\", self.dnn.summary())\n",
    "        \n",
    "    def random_seed(self, seed = 1234):\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        \n",
    "    def dnn_init(self, in_dim, out_dim, width, depth):\n",
    "        # network configuration (N: in_dim -> out_dim (in_dim -> width -> ... -> width -> out_dim))\n",
    "        network = tf.keras.Sequential()\n",
    "        network.add(tf.keras.layers.InputLayer(in_dim))\n",
    "        network.add(tf.keras.layers.Lambda(lambda x: 2. * (x - self.lb) / (self.ub - self.lb) - 1.))\n",
    "        # construct the network\n",
    "        for l in range(depth - 1):\n",
    "            network.add(tf.keras.layers.Dense(width, activation = self.activ, use_bias = True,\n",
    "                                                kernel_initializer = self.w_init, bias_initializer = self.b_init, \n",
    "                                                kernel_regularizer = None, bias_regularizer = None, \n",
    "                                                activity_regularizer = None, kernel_constraint = None, bias_constraint = None))\n",
    "        network.add(tf.keras.layers.Dense(out_dim))\n",
    "        return network\n",
    "    \n",
    "    def loss_PDE(self, t, x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            tp.watch(t)\n",
    "            tp.watch(x)\n",
    "            # u = [\\rho, v, p]\n",
    "            u = self.dnn(tf.concat([t, x], 1))\n",
    "            # \\rho_t + \\nabla*(\\rho*v) = 0\n",
    "            # (\\rho*v)_t + \\nabla*(\\rho*u^2+p) = 0\n",
    "            rho_t = tp.gradient(u[:,0],t)\n",
    "            nabla_rho_v = tp.gradient(u[:,0]*u[:,1],x)\n",
    "            rho_v_t = tp.gradient(u[:,0]*u[:,1],t)\n",
    "            nabla_rhou2_p = tp.gradient(u[:,0]*(u[:,1]**2)+u[:,2],x)\n",
    "        del tp\n",
    "        loss_f = tf.reduce_mean(tf.square(rho_t+nabla_rho_v)+tf.square(rho_v_t+nabla_rhou2_p))\n",
    "        return loss_f\n",
    "\n",
    "    def loss_nabla_rho(self,t,x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            tp.watch(t)\n",
    "            tp.watch(x)\n",
    "            u = self.dnn(tf.concat([t, x], 1))\n",
    "            rho_x = tp.gradient(u[:,0],x)\n",
    "        rho_x_real = 0.2*np.pi*tf.cos(np.pi*(x-t))\n",
    "        del tp\n",
    "        return tf.reduce_mean(tf.square(rho_x-rho_x_real))\n",
    "    \n",
    "    def loss_p_star(self,t,x,u_p):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        u_p_real = tf.convert_to_tensor(u_p,dtype = self.data_type)\n",
    "        u_nn = self.dnn(tf.concat([t, x], 1))\n",
    "        return tf.reduce_mean(tf.square(u_nn[:,0][:,None]-u_p_real))\n",
    "    \n",
    "    @tf.function\n",
    "    def grad_desc(self,\n",
    "                 t_nabla_rho, x_nabla_rho,\n",
    "                 t_p, x_p, u_p,\n",
    "                 t_f, x_f,):\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            loss = self.loss_PDE(t_f,x_f)+self.loss_nabla_rho(t_nabla_rho,x_nabla_rho)+self.loss_p_star(t_p,x_p,u_p)\n",
    "        grad = tp.gradient(loss, self.params)\n",
    "        del tp\n",
    "        self.optimizer.apply_gradients(zip(grad, self.params))\n",
    "        return loss\n",
    "        \n",
    "    def train(self, epoch = 10 ** 5, batch = 2 ** 6, tol = 1e-5): \n",
    "        print(\">>>>> training setting;\")\n",
    "        print(\"         # of epoch     :\", epoch)\n",
    "        print(\"         batch size     :\", batch)\n",
    "        print(\"         convergence tol:\", tol)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        t_f = self.t_f.numpy(); x_f = self.x_f.numpy()\n",
    "        for ep in range(epoch):\n",
    "            ep_loss = 0\n",
    "            n_r = self.x_f.shape[0]\n",
    "            idx_f = np.random.permutation(n_r)\n",
    "            for idx in range(0, n_r, batch):\n",
    "                # batch for domain residual\n",
    "                t_f_btch = tf.convert_to_tensor(t_f[idx_f[idx: idx + batch if idx + batch < n_r else n_r]], dtype = self.data_type)\n",
    "                x_f_btch = tf.convert_to_tensor(x_f[idx_f[idx: idx + batch if idx + batch < n_r else n_r]], dtype = self.data_type)\n",
    "                # compute loss and perform gradient descent\n",
    "                self.loss_PDE(t_f,x_f)\n",
    "                self.loss_nabla_rho(self.t_nabla_rho,self.x_nabla_rho)\n",
    "                self.loss_p_star(self.t_p,self.x_p,self.u_p)\n",
    "                loss_btch = self.grad_desc(self.t_nabla_rho, self.x_nabla_rho,\n",
    "                                            self.t_p, self.x_p, self.u_p,\n",
    "                                            t_f_btch, x_f_btch)\n",
    "                ep_loss += loss_btch / int(n_r / batch)\n",
    "            if ep % self.freq_info == 0:\n",
    "                elps = time.time() - t0\n",
    "                self.ep_log.append(ep)\n",
    "                self.loss_log.append(ep_loss)\n",
    "                print(\"ep: %d, loss: %.3e, elps: %.3f\" % (ep, ep_loss, elps))\n",
    "                t0 = time.time()\n",
    "            \n",
    "            if ep_loss < tol:\n",
    "                print(\">>>>> program terminating with the loss converging to its tolerance.\")\n",
    "                print(\"\\n************************************************************\")\n",
    "                print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "                print(\"************************************************************\")\n",
    "                print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                break\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                \n",
    "    def predict(self, t, x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        return self.dnn(tf.concat([t, x], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "    tmin, tmax =  0., 1.\n",
    "    xmin, xmax = -1., 1.\n",
    "    \n",
    "    N_nabla_rho = 640\n",
    "    N_p=50\n",
    "    x_star = 0.0\n",
    "    N_f = 2000\n",
    "\n",
    "    epoch = 20000\n",
    "    batch = 2**8\n",
    "    tol = 1e-6\n",
    "\n",
    "    lr0 = 4e-3\n",
    "    gam = 1e-2\n",
    "    lrd_cos = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate = lr0, \n",
    "        decay_steps = epoch, \n",
    "        alpha = gam\n",
    "        )\n",
    "    lr = lrd_cos\n",
    "\n",
    "    t_nabla_rho, x_nabla_rho, \\\n",
    "    t_p, x_p, t_f, x_f = prp_dataset(tmin, tmax, xmin, xmax,\n",
    "                                    N_nabla_rho,\n",
    "                                    N_p, x_star,\n",
    "                                    N_f)\n",
    "    u_p = func_up(t_p,x_p)\n",
    "    \n",
    "    pinn = PINN(t_nabla_rho, x_nabla_rho,\n",
    "                t_p, x_p,u_p,\n",
    "                t_f, x_f,\n",
    "                in_dim = 2, out_dim=3, width=20, depth=7, activ = \"tanh\",\n",
    "                w_init = \"glorot_normal\", b_init = \"zeros\", \n",
    "                lr = lr, opt = \"Adam\")\n",
    "\n",
    "    # if os.path.exists('./burgers_saved_model'):\n",
    "    #     # 只保留模型dnn，pinn中的其他都未保存\n",
    "    #     print('load the previous model')\n",
    "    #     pinn.dnn = tf.saved_model.load('./burgers_saved_model')   \n",
    "    # else:\n",
    "    pinn.train(epoch, batch, tol)\n",
    "    plot_loss_log(pinn.ep_log,pinn.loss_log,'train_loss_log')\n",
    "    plot_loss(pinn.ep_log,pinn.loss_log,'train_loss')\n",
    "        \n",
    "    # PINN inference\n",
    "    nt = int(1e3) + 1\n",
    "    nx = int(1e2) + 1\n",
    "    t, x, TX = prp_grd(\n",
    "        tmin, tmax, nt, \n",
    "        xmin, xmax, nx\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    u_hat = pinn.predict(t, x)\n",
    "    t1 = time.time()\n",
    "    elps = t1 - t0\n",
    "    print(\"elapsed time for PINN inference (sec):\", elps)\n",
    "    print(\"elapsed time for PINN inference (min):\", elps / 60.)\n",
    "    plot_sol1(TX, u_hat[:,0].numpy(),title='density_prediction')\n",
    "    plot_sol1(TX, u_hat[:,1].numpy(),title='velocity_prediction')\n",
    "    plot_sol1(TX, u_hat[:,2].numpy(),title='pressure_prediction')\n",
    "\n",
    "    pinn.dnn.save(\"burgers_saved_model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ca3aa8ecd0b7bddf142c4852696022db00ef254c6fe98093bb8bf9134bdc29e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
