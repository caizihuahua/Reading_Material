{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base functions (make fig & prep data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\boogie\\Reading_Material\\Learn_NNs\\myPINNs\\Appendix_A\\appendix_a.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_A/appendix_a.ipynb#ch0000001?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpolate\u001b[39;00m \u001b[39mimport\u001b[39;00m griddata\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_A/appendix_a.ipynb#ch0000001?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_A/appendix_a.ipynb#ch0000001?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_A/appendix_a.ipynb#ch0000001?line=6'>7</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_A/appendix_a.ipynb#ch0000001?line=7'>8</a>\u001b[0m \u001b[39mmake figure\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_A/appendix_a.ipynb#ch0000001?line=8'>9</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_A/appendix_a.ipynb#ch0000001?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_sol1\u001b[39m(X_star, phi1,title):\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\tensorflow\\__init__.py:473\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_current_module, \u001b[39m\"\u001b[39m\u001b[39mkeras\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    472\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m     keras\u001b[39m.\u001b[39;49m_load()\n\u001b[0;32m    474\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_module_globals[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_name] \u001b[39m=\u001b[39m module\n\u001b[0;32m     44\u001b[0m \u001b[39m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\__init__.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\models\\__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\engine\\functional.py:31\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m input_spec\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m node \u001b[39mas\u001b[39;00m node_module\n\u001b[1;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m training \u001b[39mas\u001b[39;00m training_lib\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m training_utils\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaved_model\u001b[39;00m \u001b[39mimport\u001b[39;00m json_utils\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer_utils\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_utils\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m data_adapter\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m input_layer \u001b[39mas\u001b[39;00m input_layer_module\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\engine\\compile_utils.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m losses \u001b[39mas\u001b[39;00m losses_mod\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics \u001b[39mas\u001b[39;00m metrics_mod\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m generic_utils\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m losses_utils\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\metrics\\__init__.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_metric\u001b[39;00m \u001b[39mimport\u001b[39;00m SumOverBatchSizeMetricWrapper\n\u001b[0;32m     32\u001b[0m \u001b[39m# Individual metric classes\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m MeanRelativeError\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m Accuracy\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m BinaryAccuracy\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\metrics\\metrics.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mabc\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List, Tuple, Union\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtensor\u001b[39;00m \u001b[39mimport\u001b[39;00m utils \u001b[39mas\u001b[39;00m dtensor_utils\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\activations.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[1;32m---> 20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivation\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mactivation_layers\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m deserialize_keras_object\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m serialize_keras_object\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\layers\\__init__.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_spec\u001b[39;00m \u001b[39mimport\u001b[39;00m InputSpec\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Layer\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_preprocessing_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m PreprocessingLayer\n\u001b[0;32m     29\u001b[0m \u001b[39m# Image preprocessing layers.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage_preprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m CenterCrop\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Contains the base ProcessingLayer and a subclass that uses Combiners.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mabc\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m data_adapter\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Layer\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m version_utils\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:38\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m keras_export\n\u001b[0;32m     37\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m   \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m  \u001b[39m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m   pd \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\pandas\\__init__.py:135\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomputation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39meval\u001b[39m\n\u001b[0;32m    119\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    120\u001b[0m     concat,\n\u001b[0;32m    121\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m     qcut,\n\u001b[0;32m    133\u001b[0m )\n\u001b[1;32m--> 135\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m api, arrays, errors, io, plotting, testing, tseries\n\u001b[0;32m    136\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_print_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m    138\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    139\u001b[0m     \u001b[39m# excel\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     ExcelFile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m     read_spss,\n\u001b[0;32m    169\u001b[0m )\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\pandas\\testing.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mPublic testing utility functions.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_testing\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     assert_extension_array_equal,\n\u001b[0;32m      8\u001b[0m     assert_frame_equal,\n\u001b[0;32m      9\u001b[0m     assert_index_equal,\n\u001b[0;32m     10\u001b[0m     assert_series_equal,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_extension_array_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_frame_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_series_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_index_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m ]\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\pandas\\_testing\\__init__.py:979\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpytest\u001b[39;00m\n\u001b[0;32m    976\u001b[0m     \u001b[39mreturn\u001b[39;00m pytest\u001b[39m.\u001b[39mraises(expected_exception, match\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)  \u001b[39m# noqa: PDF010\u001b[39;00m\n\u001b[1;32m--> 979\u001b[0m cython_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39mcommon\u001b[39m.\u001b[39m_cython_table\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    982\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[0;32m    983\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[39m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39m    keys and expected result.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[39m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "make figure\n",
    "'''\n",
    "def plot_sol1(X_star, phi1,title):\n",
    "    lb = X_star.min(0); ub = X_star.max(0)\n",
    "    x, y = np.linspace(lb[0], ub[0], 200), np.linspace(lb[1], ub[1], 150); x, y = np.meshgrid(x, y)\n",
    "    PHI_I = griddata(X_star, phi1.flatten(), (x, y), method = \"linear\")\n",
    "    plt.figure(figsize = (6, 8))\n",
    "    plt.imshow(PHI_I, interpolation='nearest',cmap='rainbow', extent=[0,1,-1,1], origin='lower', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('x')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    plt.savefig(f'./pics/{title}')\n",
    "\n",
    "def plot_loss_log(ep_log, loss_log,save_name):\n",
    "    plt.figure(figsize = (8, 4))\n",
    "    plt.plot(ep_log, loss_log, alpha = .7, label = \"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(alpha = .5)\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.title('train loss(log)')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    plt.savefig(f'./pics/{save_name}')\n",
    "\n",
    "def plot_loss(ep_log, loss_log,save_name):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    fig,ax = plt.subplots(2,1)\n",
    "    strt = int(len(ep_log)*0.6)\n",
    "    ax[0].plot(ep_log, loss_log)\n",
    "    ax[0].grid(alpha=.5)\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_title('train loss')\n",
    "    ax[1].plot(ep_log[strt:], loss_log[strt:])\n",
    "    ax[1].grid(alpha=.5)\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    fig.savefig(f\"./pics/{save_name}\")\n",
    "\n",
    "'''\n",
    "prepare data\n",
    "'''\n",
    "def func_u0(x):\n",
    "    return [[1.+0.2*np.sin(np.pi*a),1.,1.] for a in x.numpy().flatten()]\n",
    "\n",
    "def prp_grd(tmin, tmax, nt,\n",
    "            xmin, xmax, nx):\n",
    "    t = np.linspace(tmin, tmax, nt)\n",
    "    x = np.linspace(xmin, xmax, nx)\n",
    "    t, x = np.meshgrid(t, x)\n",
    "    t, x = t.reshape(-1, 1), x.reshape(-1, 1)\n",
    "    TX = np.c_[t, x]\n",
    "    return t, x, TX\n",
    "\n",
    "def prp_dataset(tmin, tmax, xmin, xmax, N_0, N_b, N_r):\n",
    "    lb = tf.constant([tmin, xmin], dtype = tf.float32)\n",
    "    ub = tf.constant([tmax, xmax], dtype = tf.float32)\n",
    "    t_0 = tf.ones((N_0, 1), dtype = tf.float32) * lb[0]\n",
    "    x_0 = tf.random.uniform((N_0, 1), lb[1], ub[1], dtype = tf.float32)\n",
    "    t_b = tf.random.uniform((N_b, 1), lb[0], ub[0], dtype = tf.float32)\n",
    "    x_b = [tf.ones((N_b, 1), dtype = tf.float32) * ub[1],tf.ones((N_b, 1), dtype = tf.float32) * lb[1]]\n",
    "    t_r = tf.random.uniform((N_r, 1), lb[0], ub[0], dtype = tf.float32)\n",
    "    x_r = tf.random.uniform((N_r, 1), lb[1], ub[1], dtype = tf.float32)\n",
    "\n",
    "    return t_0, x_0, t_b, x_b, t_r, x_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PINNs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 t_0, x_0, u_0, \n",
    "                 t_b, x_b,\n",
    "                 t_f, x_f, \n",
    "                 in_dim, out_dim, width, depth, activ = \"tanh\", \n",
    "                 w_init = \"glorot_normal\", b_init = \"zeros\", \n",
    "                 lr = 1e-3, opt = \"Adam\",\n",
    "                 freq_info = 100, r_seed = 1234):\n",
    "        # initialize the configuration\n",
    "        super().__init__()\n",
    "        self.r_seed = r_seed\n",
    "        self.random_seed(seed = r_seed)\n",
    "        self.data_type  = tf.float32\n",
    "        self.in_dim     = in_dim       # input dimension\n",
    "        self.out_dim     = out_dim       # output dimension\n",
    "        self.width     = width       # internal dimension\n",
    "        self.depth  = depth    # (# of hidden layers) + output layer\n",
    "        self.activ  = activ    # activation function\n",
    "        self.w_init = w_init   # initial weight\n",
    "        self.b_init = b_init   # initial bias\n",
    "        self.lr     = lr       # learning rate\n",
    "        self.opt    = opt      # name of your optimizer\n",
    "        self.freq_info = freq_info    # monitoring frequency\n",
    "\n",
    "        # input-output pair\n",
    "        self.t_0 = t_0; self.x_0 = x_0; self.u_0 = u_0   # evaluates initial condition\n",
    "        self.t_b = t_b; self.x_b = x_b;                  # evaluates boundary condition\n",
    "        self.t_f = t_f; self.x_f = x_f                   # evaluates domain residual\n",
    "        \n",
    "        # bounds\n",
    "        X_r     = tf.concat([t_f, x_f], 1)\n",
    "        self.lb = tf.cast(tf.reduce_min(X_r, axis = 0), self.data_type)\n",
    "        self.ub = tf.cast(tf.reduce_max(X_r, axis = 0), self.data_type)\n",
    "        \n",
    "        # call\n",
    "        self.dnn = self.dnn_init(in_dim, out_dim, width, depth)\n",
    "        self.params = self.dnn.trainable_variables\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = self.lr, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "        \n",
    "        # track loss\n",
    "        self.ep_log = []\n",
    "        self.loss_log = []\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"****************     MAIN PROGRAM START     ****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> start time:\", datetime.datetime.now())\n",
    "        print(\">>>>> configuration;\")\n",
    "        print(\"         dtype        :\", self.data_type)\n",
    "        print(\"         activ func   :\", self.activ)\n",
    "        print(\"         weight init  :\", self.w_init)\n",
    "        print(\"         learning rate:\", self.lr)\n",
    "        print(\"         optimizer    :\", self.opt)\n",
    "        print(\"         summary      :\", self.dnn.summary())\n",
    "        \n",
    "    def random_seed(self, seed = 1234):\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        \n",
    "    def dnn_init(self, in_dim, out_dim, width, depth):\n",
    "        # network configuration (N: in_dim -> out_dim (in_dim -> width -> ... -> width -> out_dim))\n",
    "        network = tf.keras.Sequential()\n",
    "        network.add(tf.keras.layers.InputLayer(in_dim))\n",
    "        network.add(tf.keras.layers.Lambda(lambda x: 2. * (x - self.lb) / (self.ub - self.lb) - 1.))\n",
    "        # construct the network\n",
    "        for l in range(depth - 1):\n",
    "            network.add(tf.keras.layers.Dense(width, activation = self.activ, use_bias = True,\n",
    "                                                kernel_initializer = self.w_init, bias_initializer = self.b_init, \n",
    "                                                kernel_regularizer = None, bias_regularizer = None, \n",
    "                                                activity_regularizer = None, kernel_constraint = None, bias_constraint = None))\n",
    "        network.add(tf.keras.layers.Dense(out_dim))\n",
    "        return network\n",
    "    \n",
    "    def loss_PDE(self, t, x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            tp.watch(t)\n",
    "            tp.watch(x)\n",
    "            # u = [\\rho, v, p]\n",
    "            u = self.dnn(tf.concat([t, x], 1))\n",
    "            rho = u[:,0][:,None] # tf.convert_to_tensor(u[:,0].numpy().reshape(-1,1),dtype=tf.float32)\n",
    "            v = u[:,1][:,None] #tf.convert_to_tensor(u[:,1].numpy().reshape(-1,1),dtype=tf.float32)\n",
    "            p = u[:,2][:,None] #tf.convert_to_tensor(u[:,2].numpy().reshape(-1,1),dtype=tf.float32)\n",
    "        rho_t = tp.gradient(rho,t)\n",
    "        v_t = tp.gradient(v,t)\n",
    "        rho_x = tp.gradient(rho,x)\n",
    "        v_x = tp.gradient(v,x)\n",
    "        p_x = tp.gradient(p,x)\n",
    "        equ_1 = rho_t + rho_x*v + rho*v_x\n",
    "        equ_2 = (rho_t*v + rho*v_t) + (rho*(2*v*v_x) +(v**2)*rho_x + p_x)\n",
    "        del tp\n",
    "        loss_f = tf.reduce_mean(tf.square(equ_1)+tf.square(equ_2))\n",
    "        return loss_f\n",
    "\n",
    "    def loss_bounday(self,t,x_u,x_l):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x_u = tf.convert_to_tensor(x_u, dtype = self.data_type)\n",
    "        x_l = tf.convert_to_tensor(x_l, dtype = self.data_type)\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            tp.watch(t)\n",
    "            tp.watch(x_u)\n",
    "            tp.watch(x_l)\n",
    "            u_u = self.dnn(tf.concat([t, x_u], 1))\n",
    "            u_l = self.dnn(tf.concat([t, x_l], 1))\n",
    "            rho_u = u_u[:,0]\n",
    "            v_u = u_u[:,1]\n",
    "            p_u = u_u[:,2]\n",
    "            rho_l = u_l[:,0]\n",
    "            v_l = u_l[:,1]\n",
    "            p_l = u_l[:,2]\n",
    "        rho_x_u = tp.gradient(rho_u,x_u)\n",
    "        v_x_u = tp.gradient(v_u,x_u)\n",
    "        p_x_u = tp.gradient(p_u,x_u)\n",
    "        rho_x_l = tp.gradient(rho_l,x_l)\n",
    "        v_x_l = tp.gradient(v_l,x_l)\n",
    "        p_x_l = tp.gradient(p_l,x_l)\n",
    "        nabla_u_loss = tf.square(rho_x_u-rho_x_l)+tf.square(v_x_u-v_x_l)+tf.square(p_x_u-p_x_l)\n",
    "        u_loss = tf.square(u_u[:,0][:,None]-u_l[:,0][:,None])+tf.square(u_u[:,1][:,None]-u_l[:,1][:,None])+tf.square(u_u[:,2][:,None]-u_l[:,2][:,None])\n",
    "        del tp\n",
    "        return tf.reduce_mean(nabla_u_loss) +tf.reduce_mean(u_loss)\n",
    "        \n",
    "    def loss_init(self,t,x,u_0):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        u_0 = tf.convert_to_tensor(u_0,dtype = self.data_type)\n",
    "        u = self.dnn(tf.concat([t, x], 1))\n",
    "        return tf.reduce_mean(tf.reduce_sum(tf.square(u-u_0),1))\n",
    "    \n",
    "    @tf.function\n",
    "    def grad_desc(self, \n",
    "                  t_0, x_0, u_0, \n",
    "                  t_b, x_b,\n",
    "                  t_f, x_f):\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            loss = self.loss_PDE(t_f,x_f)+self.loss_bounday(t_b,x_b[0],x_b[1])+self.loss_init(t_0,x_0,u_0)\n",
    "        grad = tp.gradient(loss, self.params)\n",
    "        del tp\n",
    "        self.optimizer.apply_gradients(zip(grad, self.params))\n",
    "        return loss\n",
    "        \n",
    "    def train(self, epoch = 10 ** 5, batch = 2 ** 6, tol = 1e-5): \n",
    "        print(\">>>>> training setting;\")\n",
    "        print(\"         # of epoch     :\", epoch)\n",
    "        print(\"         batch size     :\", batch)\n",
    "        print(\"         convergence tol:\", tol)\n",
    "        t0 = time.time()\n",
    "        t_f = self.t_f.numpy(); x_f = self.x_f.numpy()      \n",
    "        for ep in range(epoch):\n",
    "            ep_loss = 0\n",
    "            n_r = self.x_f.shape[0]\n",
    "            idx_f = np.random.permutation(n_r)\n",
    "            for idx in range(0, n_r, batch):\n",
    "                # batch for domain residual\n",
    "                t_f_btch = tf.convert_to_tensor(t_f[idx_f[idx: idx + batch if idx + batch < n_r else n_r]], dtype = self.data_type)\n",
    "                x_f_btch = tf.convert_to_tensor(x_f[idx_f[idx: idx + batch if idx + batch < n_r else n_r]], dtype = self.data_type)\n",
    "                # compute loss and perform gradient descent\n",
    "                loss_btch = self.grad_desc(self.t_0, self.x_0, self.u_0, self.t_b, self.x_b,t_f_btch, x_f_btch)\n",
    "                ep_loss += loss_btch / int(n_r / batch)\n",
    "                \n",
    "            if ep % self.freq_info == 0:\n",
    "                elps = time.time() - t0\n",
    "                self.ep_log.append(ep)\n",
    "                self.loss_log.append(ep_loss)\n",
    "                print(\"ep: %d, loss: %.3e, elps: %.3f\" % (ep, ep_loss, elps))\n",
    "                t0 = time.time()\n",
    "            \n",
    "            if ep_loss < tol:\n",
    "                print(\">>>>> program terminating with the loss converging to its tolerance.\")\n",
    "                print(\"\\n************************************************************\")\n",
    "                print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "                print(\"************************************************************\")\n",
    "                print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                break\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                \n",
    "    def predict(self, t, x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        return self.dnn(tf.concat([t, x], 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "    tmin, tmax =  0., 1.\n",
    "    xmin, xmax = -1., 1.\n",
    "    N_0 = 50\n",
    "    N_b = 50\n",
    "    N_r = 2000\n",
    "\n",
    "    epoch = 20000\n",
    "    batch = 2**8\n",
    "    tol = 1e-8\n",
    "\n",
    "    lr0 = 4e-3\n",
    "    gam = 1e-2\n",
    "    lrd_cos = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate = lr0, \n",
    "        decay_steps = epoch, \n",
    "        alpha = gam\n",
    "        )\n",
    "    lr = lrd_cos\n",
    "    t_0, x_0, t_b, x_b, t_r, x_r = prp_dataset(tmin, tmax, xmin, xmax, N_0, N_b, N_r)\n",
    "    u_0 = func_u0(x_0)\n",
    "\n",
    "    pinn = PINN(t_0, x_0, u_0, \n",
    "                t_b, x_b, \n",
    "                t_r, x_r, \n",
    "                in_dim = 2, out_dim=3, width=20, depth=7, activ = \"tanh\",\n",
    "                w_init = \"glorot_normal\", b_init = \"zeros\", \n",
    "                lr = lr, opt = \"Adam\")\n",
    "    # if os.path.exists('./example_6_saved_model'):\n",
    "    #     # 只保留模型dnn，pinn中的其他都未保存\n",
    "    #     print('load the previous model')\n",
    "    #     pinn.dnn = tf.saved_model.load('./example_saved_model')\n",
    "    # else:\n",
    "    pinn.train(epoch, batch, tol)\n",
    "    plot_loss_log(pinn.ep_log,pinn.loss_log,'train_loss_log')\n",
    "    plot_loss(pinn.ep_log,pinn.loss_log,'train_loss')\n",
    "\n",
    "    # PINN inference\n",
    "    nt = int(1e3) + 1\n",
    "    nx = int(1e2) + 1\n",
    "    t, x, TX = prp_grd(\n",
    "        tmin, tmax, nt, \n",
    "        xmin, xmax, nx\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    u_hat = pinn.predict(t, x)\n",
    "    t1 = time.time()\n",
    "    elps = t1 - t0\n",
    "    print(\"elapsed time for PINN inference (sec):\", elps)\n",
    "    print(\"elapsed time for PINN inference (min):\", elps / 60.)\n",
    "    plot_sol1(TX, u_hat[:,0].numpy(),title='density_prediction')\n",
    "    plot_sol1(TX, u_hat[:,1].numpy(),title='velocity_prediction')\n",
    "    plot_sol1(TX, u_hat[:,2].numpy(),title='pressure_prediction')\n",
    "\n",
    "    # 保存模型\n",
    "    pinn.dnn.save(\"./forward_saved_model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ca3aa8ecd0b7bddf142c4852696022db00ef254c6fe98093bb8bf9134bdc29e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
