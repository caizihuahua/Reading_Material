{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "'''\n",
    "prep data\n",
    "'''\n",
    "def func_u0(x):\n",
    "    return [[1.+0.2*np.sin(np.pi*a),1.,1.] for a in x.numpy().flatten()]\n",
    "\n",
    "def prp_grd(tmin, tmax, nt,\n",
    "            xmin, xmax, nx):\n",
    "    t = np.linspace(tmin, tmax, nt)\n",
    "    x = np.linspace(xmin, xmax, nx)\n",
    "    t, x = np.meshgrid(t, x)\n",
    "    t, x = t.reshape(-1, 1), x.reshape(-1, 1)\n",
    "    TX = np.c_[t, x]\n",
    "    return t, x, TX\n",
    "\n",
    "def prp_dataset(tmin, tmax, xmin, xmax, N_0, N_b, N_r):\n",
    "    lb = tf.constant([tmin, xmin], dtype = tf.float32)\n",
    "    ub = tf.constant([tmax, xmax], dtype = tf.float32)\n",
    "    print(\"lower bound\", lb)\n",
    "    print(\"upper bound\", ub)\n",
    "\n",
    "    t_0 = tf.ones((N_0, 1), dtype = tf.float32) * lb[0]\n",
    "    x_0 = tf.random.uniform((N_0, 1), lb[1], ub[1], dtype = tf.float32)\n",
    "    t_b = tf.random.uniform((N_b, 1), lb[0], ub[0], dtype = tf.float32)\n",
    "    x_b = [tf.ones((N_b, 1), dtype = tf.float32) * ub[1],tf.ones((N_b, 1), dtype = tf.float32) * lb[1]]\n",
    "    t_r = tf.random.uniform((N_r, 1), lb[0], ub[0], dtype = tf.float32)\n",
    "    x_r = tf.random.uniform((N_r, 1), lb[1], ub[1], dtype = tf.float32)\n",
    "\n",
    "    return t_0, x_0, t_b, x_b, t_r, x_r\n",
    "\n",
    "\n",
    "'''\n",
    "make figure\n",
    "'''\n",
    "def plot_sol1(X_star, phi1,title):\n",
    "    lb = X_star.min(0); ub = X_star.max(0)\n",
    "    x, y = np.linspace(lb[0], ub[0], 200), np.linspace(lb[1], ub[1], 150); x, y = np.meshgrid(x, y)\n",
    "    PHI_I = griddata(X_star, phi1.flatten(), (x, y), method = \"linear\")\n",
    "    plt.figure(figsize = (6, 8))\n",
    "    plt.imshow(PHI_I, interpolation='nearest',cmap='rainbow', extent=[0,1,-1,1], origin='lower', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('x')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    plt.savefig(f'./pics/{title}')\n",
    "\n",
    "def plot_loss_log(ep_log, loss_log,save_name):\n",
    "    plt.figure(figsize = (8, 4))\n",
    "    plt.plot(ep_log, loss_log, alpha = .7, label = \"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(alpha = .5)\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.title('train loss(log)')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    plt.savefig(f'./pics/{save_name}')\n",
    "\n",
    "def plot_loss(ep_log, loss_log,save_name):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    fig,ax = plt.subplots(2,1)\n",
    "    strt = int(len(ep_log)*0.6)\n",
    "    ax[0].plot(ep_log, loss_log)\n",
    "    ax[0].grid(alpha=.5)\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_title('train loss')\n",
    "    ax[1].plot(ep_log[strt:], loss_log[strt:])\n",
    "    ax[1].grid(alpha=.5)\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    fig.savefig(f\"./pics/{save_name}\")\n",
    "\n",
    "\n",
    "'''\n",
    "PINNs modle\n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 t_0, x_0, u_0, \n",
    "                 t_b, x_b,\n",
    "                 t_f, x_f, \n",
    "                 in_dim, out_dim, width, depth, activ = \"tanh\", \n",
    "                 w_init = \"glorot_normal\", b_init = \"zeros\", \n",
    "                 lr = 1e-3, opt = \"Adam\",\n",
    "                 freq_info = 10, r_seed = 1234):\n",
    "        # initialize the configuration\n",
    "        super().__init__()\n",
    "        self.r_seed = r_seed\n",
    "        self.random_seed(seed = r_seed)\n",
    "        self.data_type  = tf.float32\n",
    "        self.in_dim     = in_dim       # input dimension\n",
    "        self.out_dim     = out_dim       # output dimension\n",
    "        self.width     = width       # internal dimension\n",
    "        self.depth  = depth    # (# of hidden layers) + output layer\n",
    "        self.activ  = activ    # activation function\n",
    "        self.w_init = w_init   # initial weight\n",
    "        self.b_init = b_init   # initial bias\n",
    "        self.lr     = lr       # learning rate\n",
    "        self.opt    = opt      # name of your optimizer\n",
    "        self.freq_info = freq_info    # monitoring frequency\n",
    "\n",
    "        # input-output pair\n",
    "        self.t_0 = t_0; self.x_0 = x_0; self.u_0 = u_0   # evaluates initial condition\n",
    "        self.t_b = t_b; self.x_b = x_b;                  # evaluates boundary condition\n",
    "        self.t_f = t_f; self.x_f = x_f                   # evaluates domain residual\n",
    "        \n",
    "        # bounds\n",
    "        X_r     = tf.concat([t_f, x_f], 1)\n",
    "        self.lb = tf.cast(tf.reduce_min(X_r, axis = 0), self.data_type)\n",
    "        self.ub = tf.cast(tf.reduce_max(X_r, axis = 0), self.data_type)\n",
    "        \n",
    "        # call\n",
    "        self.dnn = self.dnn_init(in_dim, out_dim, width, depth)\n",
    "        self.params = self.dnn.trainable_variables\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = self.lr, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "        \n",
    "        # track loss\n",
    "        self.ep_log = []\n",
    "        self.loss_log = []\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"****************     MAIN PROGRAM START     ****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> start time:\", datetime.datetime.now())\n",
    "        print(\">>>>> configuration;\")\n",
    "        print(\"         dtype        :\", self.data_type)\n",
    "        print(\"         activ func   :\", self.activ)\n",
    "        print(\"         weight init  :\", self.w_init)\n",
    "        print(\"         learning rate:\", self.lr)\n",
    "        print(\"         optimizer    :\", self.opt)\n",
    "        print(\"         summary      :\", self.dnn.summary())\n",
    "        \n",
    "    def random_seed(self, seed = 1234):\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        \n",
    "    def dnn_init(self, in_dim, out_dim, width, depth):\n",
    "        # network configuration (N: in_dim -> out_dim (in_dim -> width -> ... -> width -> out_dim))\n",
    "        network = tf.keras.Sequential()\n",
    "        network.add(tf.keras.layers.InputLayer(in_dim))\n",
    "        network.add(tf.keras.layers.Lambda(lambda x: 2. * (x - self.lb) / (self.ub - self.lb) - 1.))\n",
    "        # construct the network\n",
    "        for l in range(depth - 1):\n",
    "            network.add(tf.keras.layers.Dense(width, activation = self.activ, use_bias = True,\n",
    "                                                kernel_initializer = self.w_init, bias_initializer = self.b_init, \n",
    "                                                kernel_regularizer = None, bias_regularizer = None, \n",
    "                                                activity_regularizer = None, kernel_constraint = None, bias_constraint = None))\n",
    "        network.add(tf.keras.layers.Dense(out_dim))\n",
    "        return network\n",
    "    \n",
    "    def loss_PDE(self, t, x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            tp.watch(t)\n",
    "            tp.watch(x)\n",
    "            # u = [\\rho, v, p]\n",
    "            u = self.dnn(tf.concat([t, x], 1))\n",
    "            # \\rho_t + \\nabla*(\\rho*v)\n",
    "            # (\\rho*v)_t + \\nabla*(\\rho*u^2+p)\n",
    "            rho_t = tp.gradient(u[:,0],t)\n",
    "            v_t = tp.gradient(u[:,1],t)\n",
    "            rho_x = tp.gradient(u[:,0],x)\n",
    "            v_x = tp.gradient(u[:,1],x)\n",
    "            p_x = tp.gradient(u[:,2],x)\n",
    "        rho = u[:,0][:,None] # tf.convert_to_tensor(u[:,0].numpy().reshape(-1,1),dtype=tf.float32)\n",
    "        v = u[:,1][:,None] #tf.convert_to_tensor(u[:,1].numpy().reshape(-1,1),dtype=tf.float32)\n",
    "        p = u[:,2][:,None] #tf.convert_to_tensor(u[:,2].numpy().reshape(-1,1),dtype=tf.float32)\n",
    "        equ_1 = rho_t + rho_x*v + rho*v_x\n",
    "        equ_2 = (rho_t*v + rho*v_t) + (rho*(2*v*v_x) +(v**2)*rho_x + p_x)\n",
    "        del tp\n",
    "        loss_f = tf.reduce_mean(tf.square(equ_1)+tf.square(equ_2))\n",
    "        return loss_f\n",
    "\n",
    "    def loss_bounday(self,t,x_u,x_l):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x_u = tf.convert_to_tensor(x_u, dtype = self.data_type)\n",
    "        x_l = tf.convert_to_tensor(x_l, dtype = self.data_type)\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            tp.watch(t)\n",
    "            tp.watch(x_u)\n",
    "            tp.watch(x_l)\n",
    "            u_u = self.dnn(tf.concat([t, x_u], 1))\n",
    "            u_l = self.dnn(tf.concat([t, x_l], 1))\n",
    "            rho_x_u = tp.gradient(u_u[:,0],x_u)\n",
    "            v_x_u = tp.gradient(u_u[:,1],x_u)\n",
    "            p_x_u = tp.gradient(u_u[:,2],x_u)\n",
    "            rho_x_l = tp.gradient(u_l[:,0],x_l)\n",
    "            v_x_l = tp.gradient(u_l[:,1],x_l)\n",
    "            p_x_l = tp.gradient(u_l[:,2],x_l)\n",
    "        nabla_u_loss = tf.square(rho_x_u-rho_x_l)+tf.square(v_x_u-v_x_l)+tf.square(p_x_u-p_x_l)\n",
    "        u_loss = tf.square(u_u[:,0][:,None]-u_l[:,0][:,None])+tf.square(u_u[:,1][:,None]-u_l[:,1][:,None])+tf.square(u_u[:,2][:,None]-u_l[:,2][:,None])\n",
    "        del tp\n",
    "        return tf.reduce_mean(nabla_u_loss) +tf.reduce_mean(u_loss)\n",
    "        \n",
    "    def loss_init(self,t,x,u_0):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        u_0 = tf.convert_to_tensor(u_0,dtype = self.data_type)\n",
    "        u = self.dnn(tf.concat([t, x], 1))\n",
    "        return tf.reduce_mean(tf.reduce_sum(tf.square(u-u_0),1))\n",
    "    \n",
    "    @tf.function\n",
    "    def grad_desc(self, \n",
    "                  t_0, x_0, u_0, \n",
    "                  t_b, x_b,\n",
    "                  t_f, x_f):\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            loss = self.loss_PDE(t_f,x_f)+self.loss_bounday(t_b,x_b[0],x_b[1])+self.loss_init(t_0,x_0,u_0)\n",
    "        grad = tp.gradient(loss, self.params)\n",
    "        del tp\n",
    "        self.optimizer.apply_gradients(zip(grad, self.params))\n",
    "        return loss\n",
    "        \n",
    "    def train(self, epoch = 10 ** 5, batch = 2 ** 6, tol = 1e-5): \n",
    "        print(\">>>>> training setting;\")\n",
    "        print(\"         # of epoch     :\", epoch)\n",
    "        print(\"         batch size     :\", batch)\n",
    "        print(\"         convergence tol:\", tol)\n",
    "        t0 = time.time()\n",
    "        t_f = self.t_f.numpy(); x_f = self.x_f.numpy()      \n",
    "        for ep in range(epoch):\n",
    "            ep_loss = 0\n",
    "            n_r = self.x_f.shape[0]\n",
    "            idx_f = np.random.permutation(n_r)\n",
    "            for idx in range(0, n_r, batch):\n",
    "                # batch for domain residual\n",
    "                t_f_btch = tf.convert_to_tensor(t_f[idx_f[idx: idx + batch if idx + batch < n_r else n_r]], dtype = self.data_type)\n",
    "                x_f_btch = tf.convert_to_tensor(x_f[idx_f[idx: idx + batch if idx + batch < n_r else n_r]], dtype = self.data_type)\n",
    "                # compute loss and perform gradient descent\n",
    "                loss_btch = self.grad_desc(self.t_0, self.x_0, self.u_0, self.t_b, self.x_b,t_f_btch, x_f_btch)\n",
    "                ep_loss += loss_btch / int(n_r / batch)\n",
    "                \n",
    "            if ep % self.freq_info == 0:\n",
    "                elps = time.time() - t0\n",
    "                self.ep_log.append(ep)\n",
    "                self.loss_log.append(ep_loss)\n",
    "                print(\"ep: %d, loss: %.3e, elps: %.3f\" % (ep, ep_loss, elps))\n",
    "                t0 = time.time()\n",
    "            \n",
    "            if ep_loss < tol:\n",
    "                print(\">>>>> program terminating with the loss converging to its tolerance.\")\n",
    "                print(\"\\n************************************************************\")\n",
    "                print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "                print(\"************************************************************\")\n",
    "                print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                break\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                \n",
    "    def predict(self, t, x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        return self.dnn(tf.concat([t, x], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "    tmin, tmax =  0., 1.\n",
    "    xmin, xmax = -1., 1.\n",
    "    N_0 = 50\n",
    "    N_b = 50\n",
    "    N_r = 2000\n",
    "\n",
    "    epoch = 20000\n",
    "    batch = 2**8\n",
    "    tol = 1e-7\n",
    "\n",
    "    lr0 = 4e-3\n",
    "    gam = 1e-2\n",
    "    lrd_cos = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate = lr0, \n",
    "        decay_steps = epoch, \n",
    "        alpha = gam\n",
    "        )\n",
    "    lr = lrd_cos\n",
    "    t_0, x_0, t_b, x_b, t_r, x_r = prp_dataset(tmin, tmax, xmin, xmax, N_0, N_b, N_r)\n",
    "    u_0 = func_u0(x_0)\n",
    "\n",
    "    pinn = PINN(t_0, x_0, u_0, \n",
    "                t_b, x_b, \n",
    "                t_r, x_r, \n",
    "                in_dim = 2, out_dim=3, width=20, depth=7, activ = \"tanh\",\n",
    "                w_init = \"glorot_normal\", b_init = \"zeros\", \n",
    "                lr = lr, opt = \"Adam\")\n",
    "    # if os.path.exists('./example_6_saved_model'):\n",
    "    #     # 只保留模型dnn，pinn中的其他都未保存\n",
    "    #     print('load the previous model')\n",
    "    #     pinn.dnn = tf.saved_model.load('./example_saved_model')\n",
    "    # else:\n",
    "    pinn.train(epoch, batch, tol)\n",
    "    plot_loss_log(pinn.ep_log,pinn.loss_log,'train_loss_log')\n",
    "    plot_loss(pinn.ep_log,pinn.loss_log,'train_loss')\n",
    "\n",
    "    # PINN inference\n",
    "    nt = int(1e3) + 1\n",
    "    nx = int(1e2) + 1\n",
    "    t, x, TX = prp_grd(\n",
    "        tmin, tmax, nt, \n",
    "        xmin, xmax, nx\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    u_hat = pinn.predict(t, x)\n",
    "    t1 = time.time()\n",
    "    elps = t1 - t0\n",
    "    print(\"elapsed time for PINN inference (sec):\", elps)\n",
    "    print(\"elapsed time for PINN inference (min):\", elps / 60.)\n",
    "    plot_sol1(TX, u_hat[:,0].numpy(),title='density_prediction')\n",
    "    plot_sol1(TX, u_hat[:,1].numpy(),title='velocity_prediction')\n",
    "    plot_sol1(TX, u_hat[:,2].numpy(),title='pressure_prediction')\n",
    "\n",
    "    # 保存模型\n",
    "    pinn.dnn.save(\"./forward_saved_model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
