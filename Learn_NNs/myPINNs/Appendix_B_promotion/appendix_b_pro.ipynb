{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base functions (make fig & prep data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "make figure\n",
    "'''\n",
    "def plot_sol1(X_star, phi1,title):\n",
    "    lb = X_star.min(0); ub = X_star.max(0)\n",
    "    x, y = np.linspace(lb[0], ub[0], 200), np.linspace(lb[1], ub[1], 150); x, y = np.meshgrid(x, y)\n",
    "    PHI_I = griddata(X_star, phi1.flatten(), (x, y), method = \"linear\")\n",
    "    plt.figure(figsize = (6, 8))\n",
    "    plt.imshow(PHI_I, interpolation='nearest',cmap='rainbow', extent=[0,1,-1,1], origin='lower', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('x')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    plt.savefig(f'./pics/{title}')\n",
    "\n",
    "def plot_loss_log(ep_log, loss_log,save_name):\n",
    "    plt.figure(figsize = (8, 4))\n",
    "    plt.plot(ep_log, loss_log, alpha = .7, label = \"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(alpha = .5)\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.title('train loss(log)')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    plt.savefig(f'./pics/{save_name}')\n",
    "\n",
    "def plot_loss(ep_log, loss_log,save_name):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    fig,ax = plt.subplots(2,1)\n",
    "    strt = int(len(ep_log)*0.6)\n",
    "    ax[0].plot(ep_log, loss_log)\n",
    "    ax[0].grid(alpha=.5)\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_title('train loss')\n",
    "    ax[1].plot(ep_log[strt:], loss_log[strt:])\n",
    "    ax[1].grid(alpha=.5)\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    if not os.path.exists('./pics'):\n",
    "        os.makedirs('./pics')\n",
    "    fig.savefig(f\"./pics/{save_name}\")\n",
    "\n",
    "'''\n",
    "prepare data\n",
    "'''\n",
    "\n",
    "def func_up(t,x):\n",
    "    return tf.ones((len(t.numpy()),1), dtype = tf.float32)\n",
    "\n",
    "def func_ub(x):\n",
    "    n = x.shape[0]\n",
    "    return tf.zeros((n, 1), dtype = tf.float32)\n",
    "\n",
    "def prp_grd(tmin, tmax, nt,\n",
    "            xmin, xmax, nx):\n",
    "    t = np.linspace(tmin, tmax, nt)\n",
    "    x = np.linspace(xmin, xmax, nx)\n",
    "    t, x = np.meshgrid(t, x)\n",
    "    t, x = t.reshape(-1, 1), x.reshape(-1, 1)\n",
    "    TX = np.c_[t, x]\n",
    "    return t, x, TX\n",
    "\n",
    "def prp_dataset(tmin, tmax, xmin, xmax, N_nabla_rho,N_p,x_star,N_f):\n",
    "    t_nabla_rho = tf.random.uniform((N_nabla_rho, 1), tmin, tmax, dtype = tf.float32)\n",
    "    x_nabla_rho = tf.random.uniform((N_nabla_rho,1), xmin, xmax, dtype = tf.float32)\n",
    "    t_p = tf.random.uniform((N_p, 1), tmin, tmax, dtype = tf.float32)\n",
    "    x_p = tf.ones((N_p,1),dtype=tf.float32) * x_star\n",
    "    t_f = tf.random.uniform((N_f, 1), tmin, tmax, dtype = tf.float32)\n",
    "    x_f = tf.random.uniform((N_f,1), xmin, xmax, dtype = tf.float32)\n",
    "\n",
    "    return t_nabla_rho, x_nabla_rho, t_p, x_p, t_f, x_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PINNs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 t_nabla_rho, x_nabla_rho,\n",
    "                 t_p, x_p, u_p,\n",
    "                 t_f, x_f,\n",
    "                 in_dim, out_dim, width, depth, activ = \"tanh\", \n",
    "                 w_init = \"glorot_normal\", b_init = \"zeros\", \n",
    "                 lr = 1e-3, opt = \"Adam\",\n",
    "                 freq_info = 100, r_seed = 1234):\n",
    "        # initialize the configuration\n",
    "        super().__init__()\n",
    "        self.r_seed = r_seed\n",
    "        self.random_seed(seed = r_seed)\n",
    "        self.data_type  = tf.float32\n",
    "        self.in_dim     = in_dim       # input dimension\n",
    "        self.out_dim     = out_dim       # output dimension\n",
    "        self.width     = width       # internal dimension\n",
    "        self.depth  = depth    # (# of hidden layers) + output layer\n",
    "        self.activ  = activ    # activation function\n",
    "        self.w_init = w_init   # initial weight\n",
    "        self.b_init = b_init   # initial bias\n",
    "        self.lr     = lr       # learning rate\n",
    "        self.opt    = opt      # name of your optimizer\n",
    "        self.freq_info = freq_info    # monitoring frequency\n",
    "\n",
    "        # input-output pair\n",
    "        self.t_nabla_rho = t_nabla_rho; self.x_nabla_rho = x_nabla_rho   # evaluates initial condition\n",
    "        self.t_p = t_p; self.x_p = x_p; self.u_p = u_p# evaluates boundary condition\n",
    "        self.t_f = t_f; self.x_f = x_f                   # evaluates domain residual\n",
    "        \n",
    "        # bounds\n",
    "        X_f     = tf.concat([t_f, x_f], 1)\n",
    "        self.lb = tf.cast(tf.reduce_min(X_f, axis = 0), self.data_type)\n",
    "        self.ub = tf.cast(tf.reduce_max(X_f, axis = 0), self.data_type)\n",
    "        \n",
    "        # call\n",
    "        self.dnn = self.dnn_init(in_dim, out_dim, width, depth)\n",
    "        self.params = self.dnn.trainable_variables\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = self.lr, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "\n",
    "        # track loss\n",
    "        self.ep_log = []\n",
    "        self.loss_log = []\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"****************     MAIN PROGRAM START     ****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> start time:\", datetime.datetime.now())\n",
    "        print(\">>>>> configuration;\")\n",
    "        print(\"         dtype        :\", self.data_type)\n",
    "        print(\"         activ func   :\", self.activ)\n",
    "        print(\"         weight init  :\", self.w_init)\n",
    "        print(\"         learning rate:\", self.lr)\n",
    "        print(\"         optimizer    :\", self.opt)\n",
    "        print(\"         summary      :\", self.dnn.summary())\n",
    "        \n",
    "    def random_seed(self, seed = 1234):\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        \n",
    "    def dnn_init(self, in_dim, out_dim, width, depth):\n",
    "        # network configuration (N: in_dim -> out_dim (in_dim -> width -> ... -> width -> out_dim))\n",
    "        network = tf.keras.Sequential()\n",
    "        network.add(tf.keras.layers.InputLayer(in_dim))\n",
    "        network.add(tf.keras.layers.Lambda(lambda x: 2. * (x - self.lb) / (self.ub - self.lb) - 1.))\n",
    "        # construct the network\n",
    "        for l in range(depth - 1):\n",
    "            network.add(tf.keras.layers.Dense(width, activation = self.activ, use_bias = True,\n",
    "                                                kernel_initializer = self.w_init, bias_initializer = self.b_init, \n",
    "                                                kernel_regularizer = None, bias_regularizer = None, \n",
    "                                                activity_regularizer = None, kernel_constraint = None, bias_constraint = None))\n",
    "        network.add(tf.keras.layers.Dense(out_dim))\n",
    "        return network\n",
    "    \n",
    "    def loss_PDE(self, t, x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            tp.watch(t)\n",
    "            tp.watch(x)\n",
    "            # u = [\\rho, v, p]\n",
    "            u = self.dnn(tf.concat([t, x], 1))\n",
    "            rho = u[:,0][:,None] # tf.convert_to_tensor(u[:,0].numpy().reshape(-1,1),dtype=tf.float32)\n",
    "            v = u[:,1][:,None] #tf.convert_to_tensor(u[:,1].numpy().reshape(-1,1),dtype=tf.float32)\n",
    "            p = u[:,2][:,None] #tf.convert_to_tensor(u[:,2].numpy().reshape(-1,1),dtype=tf.float32)\n",
    "        rho_t = tp.gradient(rho,t)\n",
    "        v_t = tp.gradient(v,t)\n",
    "        rho_x = tp.gradient(rho,x)\n",
    "        v_x = tp.gradient(v,x)\n",
    "        p_x = tp.gradient(p,x)\n",
    "        equ_1 = rho_t + rho_x*v + rho*v_x\n",
    "        equ_2 = (rho_t*v + rho*v_t) + (rho*(2*v*v_x) +(v**2)*rho_x + p_x)\n",
    "        del tp\n",
    "        loss_f = tf.reduce_mean(tf.square(equ_1)+tf.square(equ_2))\n",
    "        return loss_f\n",
    "\n",
    "    def loss_nabla_rho(self,t,x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            tp.watch(t)\n",
    "            tp.watch(x)\n",
    "            u = self.dnn(tf.concat([t, x], 1))\n",
    "            rho = u[:,0]\n",
    "        rho_x = tp.gradient(rho,x)\n",
    "        rho_x_real = 0.2*np.pi*tf.cos(np.pi*(x-t))\n",
    "        del tp\n",
    "        return tf.reduce_mean(tf.square(rho_x-rho_x_real))\n",
    "    # 积分精度不足\n",
    "    def loss_mass(self):\n",
    "        x = tf.convert_to_tensor(np.linspace(-1,1,1000).reshape(-1,1),dtype=tf.float32)\n",
    "        t = tf.zeros((1000,1),dtype=tf.float32)\n",
    "        rho_nn = self.dnn(tf.concat([t,x],1))[:,0][:,None]\n",
    "        int_nn = tf.reduce_mean(rho_nn)*2.\n",
    "        rho_real = tf.cast(1.0+0.2*tf.sin(np.pi*x),dtype=tf.float64)\n",
    "        int_real = tf.cast(tf.reduce_mean(rho_real)*2.,dtype=tf.float32)\n",
    "        return tf.square(int_nn-int_real)\n",
    "\n",
    "    def loss_p_star(self,t,x,u):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        u_pre = tf.convert_to_tensor(u,dtype = self.data_type)\n",
    "        u = self.dnn(tf.concat([t, x], 1))\n",
    "        return tf.reduce_mean(tf.square(u[:,2][:,None]-u_pre))+tf.reduce_mean(tf.square(u[:,1][:,None]-u_pre))\n",
    "    \n",
    "    @tf.function\n",
    "    def grad_desc(self,\n",
    "                 t_nabla_rho, x_nabla_rho,\n",
    "                 t_p, x_p, u_p,\n",
    "                 t_f, x_f,):\n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            loss = self.loss_PDE(t_f,x_f) \\\n",
    "                    +self.loss_nabla_rho(t_nabla_rho,x_nabla_rho) \\\n",
    "                    +self.loss_p_star(t_p,x_p,u_p) \\\n",
    "                    +self.loss_mass()\n",
    "        grad = tp.gradient(loss, self.params)\n",
    "        del tp\n",
    "        self.optimizer.apply_gradients(zip(grad, self.params))\n",
    "        return loss\n",
    "        \n",
    "    def train(self, epoch = 10 ** 5, batch = 2 ** 6, tol = 1e-5): \n",
    "        print(\">>>>> training setting;\")\n",
    "        print(\"         # of epoch     :\", epoch)\n",
    "        print(\"         batch size     :\", batch)\n",
    "        print(\"         convergence tol:\", tol)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        t_f = self.t_f.numpy(); x_f = self.x_f.numpy()\n",
    "        for ep in range(epoch):\n",
    "            ep_loss = 0\n",
    "            n_r = self.x_f.shape[0]\n",
    "            idx_f = np.random.permutation(n_r)\n",
    "            for idx in range(0, n_r, batch):\n",
    "                # batch for domain residual\n",
    "                \n",
    "                t_f_btch = tf.convert_to_tensor(t_f[idx_f[idx: idx + batch if idx + batch < n_r else n_r]], dtype = self.data_type)\n",
    "                x_f_btch = tf.convert_to_tensor(x_f[idx_f[idx: idx + batch if idx + batch < n_r else n_r]], dtype = self.data_type)\n",
    "                loss_btch = self.grad_desc(self.t_nabla_rho, self.x_nabla_rho,\n",
    "                                            self.t_p, self.x_p, self.u_p,\n",
    "                                            t_f_btch, x_f_btch)\n",
    "                ep_loss += loss_btch / int(n_r / batch)\n",
    "            if ep % self.freq_info == 0:\n",
    "                elps = time.time() - t0\n",
    "                self.ep_log.append(ep)\n",
    "                self.loss_log.append(ep_loss)\n",
    "                print(\"ep: %d, loss: %.3e, elps: %.3f\" % (ep, ep_loss, elps))\n",
    "                t0 = time.time()\n",
    "            \n",
    "            if ep_loss < tol:\n",
    "                print(\">>>>> program terminating with the loss converging to its tolerance.\")\n",
    "                print(\"\\n************************************************************\")\n",
    "                print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "                print(\"************************************************************\")\n",
    "                print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                break\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                \n",
    "    def predict(self, t, x):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.data_type)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.data_type)\n",
    "        return self.dnn(tf.concat([t, x], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inverse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************************************\n",
      "****************     MAIN PROGRAM START     ****************\n",
      "************************************************************\n",
      ">>>>> start time: 2022-07-05 12:12:32.185977\n",
      ">>>>> configuration;\n",
      "         dtype        : <dtype: 'float32'>\n",
      "         activ func   : tanh\n",
      "         weight init  : glorot_normal\n",
      "         learning rate: <keras.optimizers.schedules.learning_rate_schedule.CosineDecay object at 0x000002C91AF27FA0>\n",
      "         optimizer    : Adam\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda (Lambda)             (None, 2)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                60        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 63        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,223\n",
      "Trainable params: 2,223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "         summary      : None\n",
      ">>>>> training setting;\n",
      "         # of epoch     : 30000\n",
      "         batch size     : 256\n",
      "         convergence tol: 1e-08\n",
      "ep: 0, loss: 5.037e+00, elps: 3.715\n",
      "ep: 100, loss: 1.146e-03, elps: 1.815\n",
      "ep: 200, loss: 1.371e-04, elps: 1.846\n",
      "ep: 300, loss: 4.388e-05, elps: 1.851\n",
      "ep: 400, loss: 2.158e-05, elps: 1.933\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\boogie\\Reading_Material\\Learn_NNs\\myPINNs\\Appendix_B_promotion\\appendix_b_pro.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=66'>67</a>\u001b[0m     pinn\u001b[39m.\u001b[39mdnn\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mpro_saved_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=68'>69</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=69'>70</a>\u001b[0m     main()\n",
      "\u001b[1;32md:\\boogie\\Reading_Material\\Learn_NNs\\myPINNs\\Appendix_B_promotion\\appendix_b_pro.ipynb Cell 6'\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=33'>34</a>\u001b[0m pinn \u001b[39m=\u001b[39m PINN(t_nabla_rho, x_nabla_rho,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=34'>35</a>\u001b[0m             t_p, x_p,u_p,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=35'>36</a>\u001b[0m             t_f, x_f,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=36'>37</a>\u001b[0m             in_dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, out_dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, depth\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, activ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtanh\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=37'>38</a>\u001b[0m             w_init \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mglorot_normal\u001b[39m\u001b[39m\"\u001b[39m, b_init \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=38'>39</a>\u001b[0m             lr \u001b[39m=\u001b[39m lr, opt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=40'>41</a>\u001b[0m \u001b[39m# if os.path.exists('./pro_saved_model'):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=41'>42</a>\u001b[0m \u001b[39m#     # 只保留模型dnn，pinn中的其他都未保存\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=42'>43</a>\u001b[0m \u001b[39m#     print('load the previous model')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=43'>44</a>\u001b[0m \u001b[39m#     pinn.dnn = tf.saved_model.load('./pro_saved_model')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=44'>45</a>\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=45'>46</a>\u001b[0m pinn\u001b[39m.\u001b[39;49mtrain(epoch, batch, tol)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=46'>47</a>\u001b[0m plot_loss_log(pinn\u001b[39m.\u001b[39mep_log,pinn\u001b[39m.\u001b[39mloss_log,\u001b[39m'\u001b[39m\u001b[39mtrain_loss_log\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000005?line=47'>48</a>\u001b[0m plot_loss(pinn\u001b[39m.\u001b[39mep_log,pinn\u001b[39m.\u001b[39mloss_log,\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32md:\\boogie\\Reading_Material\\Learn_NNs\\myPINNs\\Appendix_B_promotion\\appendix_b_pro.ipynb Cell 4'\u001b[0m in \u001b[0;36mPINN.train\u001b[1;34m(self, epoch, batch, tol)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000003?line=166'>167</a>\u001b[0m     t_f_btch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(t_f[idx_f[idx: idx \u001b[39m+\u001b[39m batch \u001b[39mif\u001b[39;00m idx \u001b[39m+\u001b[39m batch \u001b[39m<\u001b[39m n_r \u001b[39melse\u001b[39;00m n_r]], dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_type)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000003?line=167'>168</a>\u001b[0m     x_f_btch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(x_f[idx_f[idx: idx \u001b[39m+\u001b[39m batch \u001b[39mif\u001b[39;00m idx \u001b[39m+\u001b[39m batch \u001b[39m<\u001b[39m n_r \u001b[39melse\u001b[39;00m n_r]], dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_type)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000003?line=168'>169</a>\u001b[0m     loss_btch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad_desc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_nabla_rho, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_nabla_rho,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000003?line=169'>170</a>\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_p, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_p, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mu_p,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000003?line=170'>171</a>\u001b[0m                                 t_f_btch, x_f_btch)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000003?line=171'>172</a>\u001b[0m     ep_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_btch \u001b[39m/\u001b[39m \u001b[39mint\u001b[39m(n_r \u001b[39m/\u001b[39m batch)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/Learn_NNs/myPINNs/Appendix_B_promotion/appendix_b_pro.ipynb#ch0000003?line=172'>173</a>\u001b[0m \u001b[39mif\u001b[39;00m ep \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfreq_info \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time,os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "    tmin, tmax =  0., 1.\n",
    "    xmin, xmax = -1., 1.\n",
    "    \n",
    "    N_nabla_rho = 640\n",
    "    N_p=50\n",
    "    x_star = 0.0\n",
    "    N_f = 2000\n",
    "\n",
    "    epoch = 30000\n",
    "    batch = 2**8\n",
    "    tol = 1e-8\n",
    "\n",
    "    lr0 = 4e-3\n",
    "    gam = 1e-2\n",
    "    lrd_cos = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate = lr0, \n",
    "        decay_steps = epoch, \n",
    "        alpha = gam\n",
    "        )\n",
    "    lr = lrd_cos\n",
    "\n",
    "    t_nabla_rho, x_nabla_rho, \\\n",
    "    t_p, x_p, t_f, x_f = prp_dataset(tmin, tmax, xmin, xmax,\n",
    "                                    N_nabla_rho,\n",
    "                                    N_p, x_star,\n",
    "                                    N_f)\n",
    "    u_p = func_up(t_p,x_p)\n",
    "    \n",
    "    pinn = PINN(t_nabla_rho, x_nabla_rho,\n",
    "                t_p, x_p,u_p,\n",
    "                t_f, x_f,\n",
    "                in_dim = 2, out_dim=3, width=20, depth=7, activ = \"tanh\",\n",
    "                w_init = \"glorot_normal\", b_init = \"zeros\", \n",
    "                lr = lr, opt = \"Adam\")\n",
    "\n",
    "    # if os.path.exists('./pro_saved_model'):\n",
    "    #     # 只保留模型dnn，pinn中的其他都未保存\n",
    "    #     print('load the previous model')\n",
    "    #     pinn.dnn = tf.saved_model.load('./pro_saved_model')\n",
    "    # else:\n",
    "    pinn.train(epoch, batch, tol)\n",
    "    plot_loss_log(pinn.ep_log,pinn.loss_log,'train_loss_log')\n",
    "    plot_loss(pinn.ep_log,pinn.loss_log,'train_loss')\n",
    "        \n",
    "    # PINN inference\n",
    "    nt = int(1e3) + 1\n",
    "    nx = int(1e2) + 1\n",
    "    t, x, TX = prp_grd(\n",
    "        tmin, tmax, nt, \n",
    "        xmin, xmax, nx\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    u_hat = pinn.predict(t, x)\n",
    "    t1 = time.time()\n",
    "    elps = t1 - t0\n",
    "    print(\"elapsed time for PINN inference (sec):\", elps)\n",
    "    print(\"elapsed time for PINN inference (min):\", elps / 60.)\n",
    "    plot_sol1(TX, u_hat[:,0].numpy(),title='density_prediction')\n",
    "    plot_sol1(TX, u_hat[:,1].numpy(),title='velocity_prediction')\n",
    "    plot_sol1(TX, u_hat[:,2].numpy(),title='pressure_prediction')\n",
    "\n",
    "    pinn.dnn.save(\"pro_saved_model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ca3aa8ecd0b7bddf142c4852696022db00ef254c6fe98093bb8bf9134bdc29e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
