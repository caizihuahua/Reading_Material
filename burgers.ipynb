{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import os,time,datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the PINNs\n",
    "first for burgers equation\n",
    "\n",
    "full batch\n",
    "\n",
    "2 -> 7x20 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python    : 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\n",
      "tensorflow: 2.9.1\n",
      "rand seed : 1234\n"
     ]
    }
   ],
   "source": [
    "in_dim = 2\n",
    "out_dim = 1\n",
    "width = 20\n",
    "depth = 7\n",
    "\n",
    "epoch = 8000\n",
    "tol = 1e-8\n",
    "\n",
    "N_0 = 50\n",
    "N_b = 50\n",
    "N_r = 2000\n",
    "\n",
    "w_init = \"Glorot\"\n",
    "b_init = \"zeros\"\n",
    "act = \"tanh\"\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate = 5e-3,\n",
    "    decay_steps = epoch,\n",
    "    alpha = 1e-2\n",
    ")\n",
    "\n",
    "opt = \"Adam\"\n",
    "info_freq = 100\n",
    "info_seed = 1234\n",
    "\n",
    "weight_data = 1.\n",
    "weight_pde = 1.\n",
    "\n",
    "print(\"python    :\", sys.version)\n",
    "print(\"tensorflow:\", tf.__version__)\n",
    "print(\"rand seed :\", info_seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(info_seed)\n",
    "np.random.seed(info_seed)\n",
    "tf.random.set_seed(info_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Burgers equation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&u_t+uu_x−(0.01/\\pi)u_{xx}=0,   x\\in[−1,1],   t\\in[0,1] \\\\\n",
    "&u(0,x)=−sin(\\pi x)\\\\\n",
    "&u(t,−1)=u(t,1)=0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax =  0., 1.\n",
    "xmin, xmax = -1., 1.\n",
    "lb = tf.constant([tmin, xmin], dtype = tf.float64)\n",
    "ub = tf.constant([tmax, xmax], dtype = tf.float64)\n",
    "\n",
    "t_0 = tf.ones((N_0, 1), dtype = tf.float64) * lb[0]\n",
    "x_0 = tf.random.uniform((N_0, 1), lb[1], ub[1], dtype = tf.float64)\n",
    "t_b = tf.random.uniform((N_b, 1), lb[0], ub[0], dtype = tf.float64)\n",
    "x_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b, 1), .5, dtype = tf.float64)\n",
    "t_r = tf.random.uniform((N_r, 1), lb[0], ub[0], dtype = tf.float64)\n",
    "x_r = tf.random.uniform((N_r, 1), lb[1], ub[1], dtype = tf.float64)\n",
    "\n",
    "# initial and boundary\n",
    "u_0 = -tf.sin(np.pi*x_0)\n",
    "u_b = tf.zeros((x_b.shape[0],1),dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build PINNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(tf.keras.Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            x,t,u,x_r,u_r,lb,up\n",
    "            in_dim,out_dim,width,depth,\n",
    "            activ=\"tanh\",w_init=\"glorot_normal\",b_init=\"zeros\",\n",
    "            lr=1e-3,opt=\"Adam\",weight_data=1.,weight_pde=1.,\n",
    "            info_freq=100,info_seed=1234):\n",
    "        # information\n",
    "        self.info_freq = info_freq\n",
    "        self.info_seed = info_freq\n",
    "        # initial the data\n",
    "        self.data_type = tf.float64\n",
    "        self.x = tf.convert_to_tensor(x,dtype=self.data_type)\n",
    "        self.t = tf.convert_to_tensor(t,dtype=self.data_type)\n",
    "        self.u = tf.convert_to_tensor(u,dtype=self.data_type)\n",
    "        self.x_r = tf.convert_to_tensor(x_r,dtype=self.data_type)\n",
    "        self.u_r = tf.convert_to_tensor(u_r,dtype=self.data_type)\n",
    "        self.lb = tf.convert_to_tensor(lb,dtype=self.data_type)\n",
    "        self.ub = tf.convert_to_tensor(ub,dtype=self.data_type)\n",
    "        self.nu = tf.constant(0.01/np.pi,dtype=self.data_type)\n",
    "        # neuron network configuration\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.activ = activ\n",
    "        self.w_init = w_init\n",
    "        self.b_init = b_init\n",
    "        self.lr = lr\n",
    "        self.opt = opt\n",
    "        self.weight_data = weight_data\n",
    "        self.weight_pde = weight_pde\n",
    "        \n",
    "        # call\n",
    "        self.dnn = self.dnn_init(in_dim,out_dim,width,depth)\n",
    "        self.params = self.dnn.trainable_variables\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = lr, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "\n",
    "        # track loss\n",
    "        self.ep_log = []\n",
    "        self.loss_log = []\n",
    "\n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"****************     MAIN PROGRAM START     ****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> start time:\", datetime.datetime.now())\n",
    "        print(\">>>>> configuration;\")\n",
    "        print(\"         dtype        :\", self.data_type)\n",
    "        print(\"         activ func   :\", self.activ)\n",
    "        print(\"         weight init  :\", self.w_init)\n",
    "        print(\"         learning rate:\", self.lr)\n",
    "        print(\"         optimizer    :\", self.opt)\n",
    "        print(\"         summary      :\", self.dnn.summary())\n",
    "    \n",
    "    def dnn_init(in_dim,out_dim,width,depth):\n",
    "        net = tf.keras.Sequential()\n",
    "        net.add(tf.keras.layers.InputLayer(in_dim))\n",
    "        net.add(tf.keras.layers.Lambda(lambda x: 2. * (x - self.lb) / (self.ub - self.lb) - 1.))\n",
    "\n",
    "        for l in range(depth - 1):\n",
    "            net.add(tf.keras.layers.Dense(units=width, activation = activ,kernel_initializer = w_init, bias_initializer = b_init, ))\n",
    "        net.add(tf.keras.layers.Dense(out_dim))\n",
    "        return net\n",
    "    \n",
    "    def loss_pde(self):\n",
    "        # with tf.GradientTape(persistent=True) as tp\n",
    "        with tf.GradientTape() as tp:\n",
    "            with tf.GradientTape() as tp2:\n",
    "                tp.watch(self.x_r)\n",
    "                tp.watch(self.t_r)\n",
    "                u = self.dnn(tf.concat([self.t_r,self.x_r],1))\n",
    "            u_t = tp2.gradient(u,t)\n",
    "            u_x = tp2.gradient(u,x)\n",
    "        u_xx = tp.gradient(u_x,x)\n",
    "        del tp,tp2\n",
    "        gv = u_t + u * u_x - self.nu * u_xx\n",
    "        r = tf.reduce_mean(tf.square(gv))\n",
    "        return r\n",
    "\n",
    "    def loss_icbc(self):\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2.], shape=(2,), dtype=float32)\n",
      "tf.Tensor([1. 8.], shape=(2,), dtype=float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\boogie\\Reading_Material\\burgers.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000007?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000007?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(z)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000007?line=8'>9</a>\u001b[0m dz \u001b[39m=\u001b[39m tp\u001b[39m.\u001b[39mgradient(z,x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000007?line=9'>10</a>\u001b[0m ddz \u001b[39m=\u001b[39m tp\u001b[39m.\u001b[39mjacobian(z,x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000007?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(dz)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tp' is not defined"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor([1.,2.])\n",
    "# A = tf.convert_to_tensor([1.,2.])\n",
    "with tf.GradientTape(persistent=True) as tp:\n",
    "    tp.watch(x)\n",
    "    z = x**3\n",
    "del tp\n",
    "print(x)\n",
    "print(z)\n",
    "dz = tp.gradient(z,x)\n",
    "ddz = tp.jacobian(z,x)\n",
    "print(dz)\n",
    "print(ddz)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ca3aa8ecd0b7bddf142c4852696022db00ef254c6fe98093bb8bf9134bdc29e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
