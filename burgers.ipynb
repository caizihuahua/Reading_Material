{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,time,datetime,sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the PINNs\n",
    "first for burgers equation\n",
    "\n",
    "full batch\n",
    "\n",
    "2 -> 7x20 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python    : 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\n",
      "tensorflow: 2.9.1\n",
      "rand seed : 1234\n"
     ]
    }
   ],
   "source": [
    "in_dim = 2\n",
    "out_dim = 1\n",
    "width = 20\n",
    "depth = 7\n",
    "\n",
    "epoch = 8000\n",
    "tol = 1e-8\n",
    "\n",
    "N_0 = 50\n",
    "N_b = 50\n",
    "N_r = 2000\n",
    "\n",
    "w_init = \"glorot_normal\"\n",
    "b_init = \"zeros\"\n",
    "act = \"tanh\"\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate = 5e-3,\n",
    "    decay_steps = epoch,\n",
    "    alpha = 1e-2\n",
    ")\n",
    "\n",
    "opt = \"Adam\"\n",
    "info_freq = 100\n",
    "info_seed = 1234\n",
    "\n",
    "weight_data = 1.\n",
    "weight_pde = 1.\n",
    "\n",
    "print(\"python    :\", sys.version)\n",
    "print(\"tensorflow:\", tf.__version__)\n",
    "print(\"rand seed :\", info_seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(info_seed)\n",
    "np.random.seed(info_seed)\n",
    "tf.random.set_seed(info_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Burgers equation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&u_t+uu_x−(0.01/\\pi)u_{xx}=0,   x\\in[−1,1],   t\\in[0,1] \\\\\n",
    "&u(0,x)=−sin(\\pi x)\\\\\n",
    "&u(t,−1)=u(t,1)=0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax =  0., 1.\n",
    "xmin, xmax = -1., 1.\n",
    "lb = tf.constant([tmin, xmin], dtype = tf.float32)\n",
    "ub = tf.constant([tmax, xmax], dtype = tf.float32)\n",
    "\n",
    "t_0 = tf.ones((N_0, 1), dtype = tf.float32) * lb[0]\n",
    "x_0 = tf.random.uniform((N_0, 1), lb[1], ub[1], dtype = tf.float32)\n",
    "t_b = tf.random.uniform((N_b, 1), lb[0], ub[0], dtype = tf.float32)\n",
    "x_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b, 1), .5, dtype = tf.float32)\n",
    "t_r = tf.random.uniform((N_r, 1), lb[0], ub[0], dtype = tf.float32)\n",
    "x_r = tf.random.uniform((N_r, 1), lb[1], ub[1], dtype = tf.float32)\n",
    "\n",
    "# initial and boundary\n",
    "u_0 = -tf.sin(np.pi*x_0)\n",
    "u_b = tf.zeros((x_b.shape[0],1),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.concat([t_0,t_b],axis=0)\n",
    "x = tf.concat([x_0,x_b],axis=0)\n",
    "u = tf.concat([u_0,u_b],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build PINNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(tf.keras.Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            t,x,u,t_r,x_r,lb,ub,\n",
    "            in_dim,out_dim,width,depth,\n",
    "            activ=\"tanh\",w_init=\"glorot_normal\",b_init=\"zeros\",\n",
    "            lr=1e-3,opt=\"Adam\",weight_data=1.,weight_pde=1.,\n",
    "            info_freq=100,info_seed=1234):\n",
    "        super().__init__()\n",
    "        # information\n",
    "        self.info_freq = info_freq\n",
    "        self.info_seed = info_freq\n",
    "        # initial the data\n",
    "        self.data_type = tf.float32\n",
    "        self.x = tf.convert_to_tensor(x,dtype=self.data_type)\n",
    "        self.t = tf.convert_to_tensor(t,dtype=self.data_type)\n",
    "        self.u = tf.convert_to_tensor(u,dtype=self.data_type)\n",
    "        self.x_r = tf.convert_to_tensor(x_r,dtype=self.data_type)\n",
    "        self.t_r = tf.convert_to_tensor(t_r,dtype=self.data_type)\n",
    "        self.lb = tf.convert_to_tensor(lb,dtype=self.data_type)\n",
    "        self.ub = tf.convert_to_tensor(ub,dtype=self.data_type)\n",
    "        self.nu = tf.constant(0.01/np.pi,dtype=self.data_type)\n",
    "        # neuron network configuration\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.activ = activ\n",
    "        self.w_init = w_init\n",
    "        self.b_init = b_init\n",
    "        self.lr = lr\n",
    "        self.opt = opt\n",
    "        self.weight_data = weight_data\n",
    "        self.weight_pde = weight_pde\n",
    "        \n",
    "        # call\n",
    "        self.dnn = self.dnn_init(in_dim,out_dim,width,depth)\n",
    "        self.params = self.dnn.trainable_variables\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = lr, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "\n",
    "        # track loss\n",
    "        self.ep_log = []\n",
    "        self.loss_log = []\n",
    "\n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"****************     MAIN PROGRAM START     ****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> start time:\", datetime.datetime.now())\n",
    "        print(\">>>>> configuration;\")\n",
    "        print(\"         dtype        :\", self.data_type)\n",
    "        print(\"         activ func   :\", self.activ)\n",
    "        print(\"         weight init  :\", self.w_init)\n",
    "        print(\"         learning rate:\", self.lr)\n",
    "        print(\"         optimizer    :\", self.opt)\n",
    "        print(\"         summary      :\", self.dnn.summary())\n",
    "    \n",
    "    def dnn_init(self,in_dim,out_dim,width,depth):\n",
    "        net = tf.keras.Sequential()\n",
    "        net.add(tf.keras.layers.InputLayer(in_dim))\n",
    "        # net.add(tf.keras.layers.Lambda(lambda x: 2. * (x - self.lb) / (self.ub - self.lb) - 1.))\n",
    "\n",
    "        for l in range(depth - 1):\n",
    "            net.add(tf.keras.layers.Dense(units=width, activation = self.activ,kernel_initializer = self.w_init, bias_initializer = self.b_init, ))\n",
    "        net.add(tf.keras.layers.Dense(out_dim))\n",
    "        return net\n",
    "    \n",
    "    def loss_pde(self):\n",
    "        with tf.GradientTape(persistent=True) as tp:\n",
    "            tp.watch(self.t_r)\n",
    "            tp.watch(self.x_r)\n",
    "            u = self.dnn(tf.concat([self.t_r,self.x_r],1))\n",
    "            u_t = tp.gradient(u,self.t_r)\n",
    "            u_x = tp.gradient(u,self.x_r)\n",
    "        u_xx = tp.gradient(u_x,self.x_r)\n",
    "        del tp\n",
    "        gv = u_t + u * u_x - self.nu * u_xx\n",
    "        r = tf.reduce_mean(tf.square(gv))\n",
    "        return r\n",
    "    \n",
    "    def loss_icbc(self):\n",
    "        u_nn = self.dnn(tf.concat([self.t,self.x],1))\n",
    "        return tf.reduce_mean(tf.square(self.u-u_nn))\n",
    "    \n",
    "    @tf.function\n",
    "    def grad_desc(self):\n",
    "        with tf.GradientTape() as tp:\n",
    "            loss = self.loss_pde() + self.loss_icbc()\n",
    "        grad = tp.gradient(loss,self.params)\n",
    "        del tp\n",
    "        self.optimizer.apply_gradients(zip(grad,self.params))\n",
    "        return loss\n",
    "    \n",
    "    def train(self,epoch,tol):\n",
    "        print(\">>>>> training setting;\")\n",
    "        print(\"         # of epoch     :\", epoch)\n",
    "        print(\"         convergence tol:\", tol)\n",
    "\n",
    "        t0 = time.time()\n",
    "        for ep in range(epoch):\n",
    "            self.loss_pde()\n",
    "            self.loss_icbc()\n",
    "            ep_loss = self.grad_desc()\n",
    "            if ep % self.info_freq ==0:\n",
    "                elps = time.time() -t0\n",
    "                self.ep_log.append(ep)\n",
    "                self.loss_log.append(ep_loss)\n",
    "                print(\"ep: %d, loss: %.3e, elps: %.3f\" % (ep, ep_loss, elps))\n",
    "                t0 = time.time()\n",
    "            if ep_loss < tol:\n",
    "                print(\">>>>> program terminating with the loss converging to its tolerance.\")\n",
    "                print(\"\\n************************************************************\")\n",
    "                print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "                print(\"************************************************************\")\n",
    "                print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                break\n",
    "        \n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> end time:\", datetime.datetime.now())\n",
    "\n",
    "    def predict(self,t,x):\n",
    "        with tf.GradientTape() as tp:\n",
    "            with tf.GradientTape() as tp2:\n",
    "                tp.watch(t)\n",
    "                tp.watch(x)\n",
    "                u = self.dnn(tf.concat([t,x],1))\n",
    "            u_t = tp2.gradient(u,t)\n",
    "            u_x = tp2.gradient(u,x)\n",
    "        u_xx = tp.gradient(u_x,x)\n",
    "        del tp,tp2\n",
    "        gv = u_t + u * u_x - self.nu * u_xx\n",
    "        r = tf.reduce_mean(tf.square(gv))\n",
    "        return u,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************************************\n",
      "****************     MAIN PROGRAM START     ****************\n",
      "************************************************************\n",
      ">>>>> start time: 2022-07-13 11:06:23.622448\n",
      ">>>>> configuration;\n",
      "         dtype        : <dtype: 'float32'>\n",
      "         activ func   : tanh\n",
      "         weight init  : glorot_normal\n",
      "         learning rate: 0.001\n",
      "         optimizer    : Adam\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 20)                60        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,181\n",
      "Trainable params: 2,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "         summary      : None\n"
     ]
    }
   ],
   "source": [
    "pinn = PINN(t,x,u,t_r,x_r,lb,ub,\n",
    "            in_dim,out_dim,width,depth,\n",
    "            activ=act,w_init=w_init,b_init=b_init,\n",
    "            lr=1e-3,opt=opt,weight_data=1.,weight_pde=1.,\n",
    "            info_freq=100,info_seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> training setting;\n",
      "         # of epoch     : 8000\n",
      "         convergence tol: 1e-08\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "ep: 0, loss: 3.361e-01, elps: 1.210\n",
      "ep: 100, loss: 1.187e-01, elps: 2.539\n",
      "ep: 200, loss: 9.691e-02, elps: 2.389\n",
      "ep: 300, loss: 8.662e-02, elps: 2.411\n",
      "ep: 400, loss: 7.274e-02, elps: 2.415\n",
      "ep: 500, loss: 6.694e-02, elps: 2.454\n",
      "ep: 600, loss: 6.243e-02, elps: 2.456\n",
      "ep: 700, loss: 5.637e-02, elps: 2.450\n",
      "ep: 800, loss: 4.980e-02, elps: 2.415\n",
      "ep: 900, loss: 4.427e-02, elps: 2.405\n",
      "ep: 1000, loss: 4.333e-02, elps: 2.505\n",
      "ep: 1100, loss: 4.054e-02, elps: 2.615\n",
      "ep: 1200, loss: 3.844e-02, elps: 2.611\n",
      "ep: 1300, loss: 3.754e-02, elps: 2.612\n",
      "ep: 1400, loss: 3.371e-02, elps: 2.613\n",
      "ep: 1500, loss: 3.148e-02, elps: 2.604\n",
      "ep: 1600, loss: 3.022e-02, elps: 2.631\n",
      "ep: 1700, loss: 2.822e-02, elps: 2.654\n",
      "ep: 1800, loss: 2.715e-02, elps: 2.639\n",
      "ep: 1900, loss: 2.568e-02, elps: 2.622\n",
      "ep: 2000, loss: 2.416e-02, elps: 2.621\n",
      "ep: 2100, loss: 5.159e-02, elps: 2.638\n",
      "ep: 2200, loss: 2.176e-02, elps: 2.672\n",
      "ep: 2300, loss: 2.098e-02, elps: 2.677\n",
      "ep: 2400, loss: 1.907e-02, elps: 2.669\n",
      "ep: 2500, loss: 1.852e-02, elps: 2.683\n",
      "ep: 2600, loss: 1.736e-02, elps: 2.824\n",
      "ep: 2700, loss: 1.575e-02, elps: 2.714\n",
      "ep: 2800, loss: 1.479e-02, elps: 2.764\n",
      "ep: 2900, loss: 1.177e-02, elps: 2.732\n",
      "ep: 3000, loss: 1.100e-02, elps: 2.733\n",
      "ep: 3100, loss: 1.013e-02, elps: 2.722\n",
      "ep: 3200, loss: 9.403e-03, elps: 2.783\n",
      "ep: 3300, loss: 8.927e-03, elps: 2.762\n",
      "ep: 3400, loss: 8.414e-03, elps: 2.747\n",
      "ep: 3500, loss: 7.818e-03, elps: 2.755\n",
      "ep: 3600, loss: 7.781e-03, elps: 2.761\n",
      "ep: 3700, loss: 7.262e-03, elps: 2.758\n",
      "ep: 3800, loss: 6.074e-03, elps: 2.813\n",
      "ep: 3900, loss: 5.641e-03, elps: 2.787\n",
      "ep: 4000, loss: 7.109e-03, elps: 2.785\n",
      "ep: 4100, loss: 5.813e-03, elps: 2.829\n",
      "ep: 4200, loss: 5.695e-02, elps: 2.789\n",
      "ep: 4300, loss: 2.735e-02, elps: 2.819\n",
      "ep: 4400, loss: 1.996e-02, elps: 2.839\n",
      "ep: 4500, loss: 1.684e-02, elps: 2.830\n",
      "ep: 4600, loss: 1.519e-02, elps: 2.829\n",
      "ep: 4700, loss: 1.400e-02, elps: 2.835\n",
      "ep: 4800, loss: 1.264e-02, elps: 2.878\n",
      "ep: 4900, loss: 1.168e-02, elps: 2.943\n",
      "ep: 5000, loss: 1.124e-02, elps: 2.887\n",
      "ep: 5100, loss: 1.092e-02, elps: 2.890\n",
      "ep: 5200, loss: 1.067e-02, elps: 2.908\n",
      "ep: 5300, loss: 1.045e-02, elps: 2.888\n",
      "ep: 5400, loss: 1.026e-02, elps: 2.883\n",
      "ep: 5500, loss: 1.007e-02, elps: 2.890\n",
      "ep: 5600, loss: 9.903e-03, elps: 2.874\n",
      "ep: 5700, loss: 9.738e-03, elps: 2.892\n",
      "ep: 5800, loss: 9.577e-03, elps: 2.891\n",
      "ep: 5900, loss: 9.420e-03, elps: 2.888\n",
      "ep: 6000, loss: 9.265e-03, elps: 2.933\n",
      "ep: 6100, loss: 9.114e-03, elps: 2.879\n",
      "ep: 6200, loss: 8.966e-03, elps: 2.864\n",
      "ep: 6300, loss: 8.822e-03, elps: 2.885\n",
      "ep: 6400, loss: 8.682e-03, elps: 2.878\n",
      "ep: 6500, loss: 8.546e-03, elps: 2.896\n",
      "ep: 6600, loss: 8.413e-03, elps: 2.892\n",
      "ep: 6700, loss: 8.284e-03, elps: 2.875\n",
      "ep: 6800, loss: 8.157e-03, elps: 2.892\n",
      "ep: 6900, loss: 8.033e-03, elps: 2.882\n",
      "ep: 7000, loss: 7.910e-03, elps: 2.912\n",
      "ep: 7100, loss: 7.789e-03, elps: 2.898\n",
      "ep: 7200, loss: 9.524e-03, elps: 2.930\n",
      "ep: 7300, loss: 7.569e-03, elps: 2.902\n",
      "ep: 7400, loss: 7.482e-03, elps: 2.888\n",
      "ep: 7500, loss: 7.374e-03, elps: 2.908\n",
      "ep: 7600, loss: 7.269e-03, elps: 2.882\n",
      "ep: 7700, loss: 7.185e-03, elps: 2.888\n",
      "ep: 7800, loss: 7.181e-03, elps: 2.882\n",
      "ep: 7900, loss: 7.086e-03, elps: 2.895\n",
      "\n",
      "************************************************************\n",
      "*****************     MAIN PROGRAM END     *****************\n",
      "************************************************************\n",
      ">>>>> end time: 2022-07-13 11:10:05.553238\n"
     ]
    }
   ],
   "source": [
    "pinn.train(epoch,tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AxesSubplot' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\boogie\\Reading_Material\\burgers.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000016?line=44'>45</a>\u001b[0m x\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m,\u001b[39m7\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000016?line=45'>46</a>\u001b[0m y\u001b[39m=\u001b[39m[\u001b[39m100\u001b[39m,\u001b[39m200\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000016?line=47'>48</a>\u001b[0m plot_loss(pinn\u001b[39m.\u001b[39;49mep_log,pinn\u001b[39m.\u001b[39;49mloss_log,\u001b[39m\"\u001b[39;49m\u001b[39mnothing\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32md:\\boogie\\Reading_Material\\burgers.ipynb Cell 12\u001b[0m in \u001b[0;36mplot_loss\u001b[1;34m(x, y, title, savepath)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000016?line=3'>4</a>\u001b[0m fig,ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000016?line=4'>5</a>\u001b[0m strt \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(x)\u001b[39m*\u001b[39m\u001b[39m0.7\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000016?line=5'>6</a>\u001b[0m ax[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mplot(x,y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000016?line=6'>7</a>\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mgrid(alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/boogie/Reading_Material/burgers.ipynb#ch0000016?line=7'>8</a>\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_ylabel(\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'AxesSubplot' object is not subscriptable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(x,y,title,savepath=\"./pics\"):\n",
    "    # plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=1)\n",
    "    # plt.figure(figsize=(8,4))\n",
    "    fig1,ax1 = plt.subplots(1)\n",
    "    strt = int(len(x)*0.7)\n",
    "    ax1.plot(x,y)\n",
    "    ax1.grid(alpha=0.5)\n",
    "    ax1.set_ylabel(\"loss\")\n",
    "    ax1.set_xlabel(\"epoch\")\n",
    "    ax1.set_title(\"train loss\")\n",
    "\n",
    "    # plt.figure(figsize=(8,4))\n",
    "    fig2,ax2 = plt.subplots(1)\n",
    "    ax2.plot(x,y)\n",
    "    ax2.grid(alpha=0.5)\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.set_ylabel(\"loss\")\n",
    "    ax2.set_xlabel(\"epoch\")\n",
    "    ax1.set_title(\"train loss(log)\")\n",
    "\n",
    "    # plt.figure(figsize=(8,4))\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    strt = int(len(x)*0.7)\n",
    "    ax3.plot(x[strt:],y[strt:])\n",
    "    ax3.grid(alpha=0.5)\n",
    "    ax3.set_xlabel(\"epoch\")\n",
    "    ax3.set_ylabel(\"loss\")\n",
    "    ax3.set_xlabel(\"epoch\")\n",
    "    ax3.set_title(\"train loss(part)\")\n",
    "\n",
    "    if not os.path.exists(savepath):\n",
    "        os.makedirs(savepath)\n",
    "    fig1.savefig(savepath+\"/\"+title)\n",
    "    fig2.savefig(savepath+\"/\"+title+\"(log)\")\n",
    "    fig3.savefig(savepath+\"/\"+title+\"(part\")\n",
    "\n",
    "x=[1,2,3,4,5,6,7]\n",
    "y=[100,200,5,4,3,2,1]\n",
    "\n",
    "plot_loss(pinn.ep_log,pinn.loss_log,\"nothing\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ca3aa8ecd0b7bddf142c4852696022db00ef254c6fe98093bb8bf9134bdc29e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
